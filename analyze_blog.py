import os
import requests
from bs4 import BeautifulSoup
from janome.tokenizer import Tokenizer
from collections import Counter
import re
from datetime import datetime, timedelta, timezone
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import argparse

def load_stopwords(filepath='stopwords.txt'):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿ã€ã‚»ãƒƒãƒˆã¨ã—ã¦è¿”ã™"""
    if not os.path.exists(filepath):
        print(f"è­¦å‘Š: é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ« '{filepath}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return set()
    with open(filepath, 'r', encoding='utf-8') as f:
        stopwords = {line.strip() for line in f if line.strip()}
    print(f"{len(stopwords)}å€‹ã®é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ã‚’ '{filepath}' ã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚")
    return stopwords

def analyze_blog(base_url, days_to_check, stopwords):
    # ... (ã“ã®é–¢æ•°ã®å†…å®¹ã¯Colabç‰ˆã¨å…¨ãåŒã˜) ...
    all_text = ""
    jst = timezone(timedelta(hours=9))
    start_date = datetime.now(jst) - timedelta(days=days_to_check)
    current_url = base_url
    page_num = 1
    print(f"ãƒ–ãƒ­ã‚°ã®åˆ†æã‚’é–‹å§‹ã—ã¾ã™ã€‚å¯¾è±¡æœŸé–“: {days_to_check}æ—¥é–“")
    while current_url:
        print(f"ãƒšãƒ¼ã‚¸ {page_num} ã‚’å–å¾—ä¸­...")
        try:
            response = requests.get(current_url)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
        except:
            print("ã‚¨ãƒ©ãƒ¼: ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            break
        posts = soup.find_all('article', class_='post-outer-container')
        if not posts and page_num == 1: return None
        page_contains_valid_posts = False
        for post in posts:
            time_tag = post.find('time', class_='published')
            body_tag = post.find('div', class_='post-body')
            if not time_tag or not body_tag: continue
            post_date = datetime.fromisoformat(time_tag['datetime'])
            if post_date >= start_date:
                page_contains_valid_posts = True
                all_text += body_tag.get_text() + "\n"
        if not page_contains_valid_posts and page_num > 1: break
        older_posts_link = soup.find('a', class_='blog-pager-older-link')
        if older_posts_link and older_posts_link.has_attr('href'):
            current_url = older_posts_link['href']
            page_num += 1
        else:
            current_url = None
    if not all_text: return None
    print("\nãƒ†ã‚­ã‚¹ãƒˆã®è§£æä¸­...")
    t = Tokenizer()
    words = [token.surface for token in t.tokenize(all_text) 
             if token.surface not in stopwords and 
             token.part_of_speech.startswith(('åè©', 'å‹•è©', 'å½¢å®¹è©')) and 
             len(token.surface) > 1 and not re.match(r'^[0-9a-zA-Z]+$', token.surface)]
    return Counter(words)

def create_word_cloud(counter, font_path):
    # ... (ã“ã®é–¢æ•°ã®å†…å®¹ã¯Colabç‰ˆã¨å…¨ãåŒã˜) ...
    if not font_path:
        print("\nâš ï¸  ã‚¨ãƒ©ãƒ¼: --font ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        return
    if not os.path.exists(font_path):
        print(f"\nã‚¨ãƒ©ãƒ¼: æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {font_path}")
        return
    print("\nãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”»åƒã‚’ç”Ÿæˆä¸­...")
    try:
        wordcloud = WordCloud(
            width=1200, height=600, background_color='white',
            font_path=font_path, max_words=150, colormap='viridis'
        ).generate_from_frequencies(counter)
        output_filename = "blog_wordcloud_local.png"
        wordcloud.to_file(output_filename)
        print(f"\nğŸ‰ å®Œæˆï¼ '{output_filename}' ã¨ã„ã†åå‰ã§ç”»åƒã‚’ä¿å­˜ã—ã¾ã—ãŸï¼")
    except Exception as e:
         print(f"ã‚¨ãƒ©ãƒ¼: ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚: {e}")

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Bloggerã®å˜èªé »åº¦ã‚’åˆ†æã—ã€ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆã—ã¾ã™ã€‚')
    parser.add_argument('--url', type=str, required=True, help='åˆ†æã—ãŸã„Bloggerã®URL')
    parser.add_argument('--days', type=int, default=30, help='åˆ†æå¯¾è±¡ã¨ã™ã‚‹æ—¥æ•°')
    parser.add_argument('--font', type=str, required=True, help='(é‡è¦)æ—¥æœ¬èªè¡¨ç¤ºã®ãŸã‚ã®ãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
    parser.add_argument('--stopwords', type=str, default='stopwords.txt', help='é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    
    args = parser.parse_args()
    
    custom_stopwords = load_stopwords(args.stopwords)
    word_counter = analyze_blog(args.url, args.days, custom_stopwords)
    
    if word_counter:
        print("\n--- âœ…å˜èªã®é »åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚° TOP 50 ---")
        for word, count in word_counter.most_common(50):
            print(f"{word}: {count}å›")
        
        create_word_cloud(word_counter, args.font)
