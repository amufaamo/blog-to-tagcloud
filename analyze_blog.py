import requests
from bs4 import BeautifulSoup
from janome.tokenizer import Tokenizer
from collections import Counter
import re
from datetime import datetime, timedelta, timezone
import argparse
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# --- â–¼â–¼â–¼ ã“ã“ãŒæ–°ã—ã„ãƒã‚¤ãƒ³ãƒˆï¼ â–¼â–¼â–¼ ---
# é™¤å¤–ã—ãŸã„å˜èªã‚’ã“ã®ãƒªã‚¹ãƒˆã«è¿½åŠ ã—ã¦ãã ã•ã„
CUSTOM_STOP_WORDS = {
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæŒ‡å®šã—ãŸå˜èª
    'ã™ã‚‹', 'ã„ã‚‹', 
    
    # ãã®ä»–ã€ä¸€èˆ¬çš„ã™ãã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹å˜èªã®ä¾‹
    'ã‚ã‚‹', 'ãªã„', 'æ€ã†', 'ã“ã¨', 'ã‚‚ã®',
    'ãªã‚‹', 'ã‚ˆã†', 'ã¿ãŸã„', 'ã„ã†', 'ã“ã‚Œ',
    'ãã‚Œ', 'ã„ã„', 'ã•ã‚“', 'ã¡ã‚ƒã‚“', 'ãã‚“'
}
# --- â–²â–²â–² ã“ã“ã¾ã§ â–²â–²â–² ---


def analyze_blog(base_url, days_to_check):
    """ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚’åé›†ãƒ»è§£æã—ã¦å˜èªã®é »å‡ºåº¦ã‚’è¿”ã™"""
    all_text = ""
    jst = timezone(timedelta(hours=9))
    start_date = datetime.now(jst) - timedelta(days=days_to_check)
    current_url = base_url
    page_num = 1
    
    print(f"ãƒ–ãƒ­ã‚°ã®åˆ†æã‚’é–‹å§‹ã—ã¾ã™ã€‚å¯¾è±¡æœŸé–“: {days_to_check}æ—¥é–“")
    print(f"åˆ†æé–‹å§‹æ—¥: {start_date.strftime('%Y-%m-%d')}")

    while current_url:
        print(f"\nãƒšãƒ¼ã‚¸ {page_num} ã‚’å–å¾—ä¸­...: {current_url}")
        try:
            response = requests.get(current_url)
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.text, 'html.parser')
        except requests.exceptions.RequestException as e:
            print(f"ã‚¨ãƒ©ãƒ¼: ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ {e}")
            break
        
        posts = soup.find_all('article', class_='post-outer-container')
        if not posts and page_num == 1:
            return None

        page_contains_valid_posts = False
        for post in posts:
            time_tag = post.find('time', class_='published')
            body_tag = post.find('div', class_='post-body')

            if not time_tag or not body_tag:
                continue
            
            post_date = datetime.fromisoformat(time_tag['datetime'])
            
            if post_date >= start_date:
                page_contains_valid_posts = True
                all_text += body_tag.get_text() + "\n"
        
        if not page_contains_valid_posts and page_num > 1:
            break

        older_posts_link = soup.find('a', class_='blog-pager-older-link')
        if older_posts_link and older_posts_link.has_attr('href'):
            current_url = older_posts_link['href']
            page_num += 1
        else:
            current_url = None
    
    if not all_text:
        print("ã‚¨ãƒ©ãƒ¼: åˆ†æå¯¾è±¡ã¨ãªã‚‹ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None

    print("\nåé›†ã—ãŸã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è§£æä¸­...")
    t = Tokenizer()
    words = []
    for token in t.tokenize(all_text):
        word = token.surface
        part_of_speech = token.part_of_speech.split(',')[0]
        
        # âœ… æ¡ä»¶ã‚’è¿½åŠ ï¼šé™¤å¤–ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œã¦ã„ãªã„å˜èªã®ã¿ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        if word not in CUSTOM_STOP_WORDS and \
           part_of_speech in ['åè©', 'å‹•è©', 'å½¢å®¹è©'] and \
           len(word) > 1 and \
           not re.match(r'^[0-9]+$', word):
            words.append(word)
    
    return Counter(words)

def create_word_cloud(counter, font_path):
    """å˜èªã®é »åº¦æƒ…å ±ã‹ã‚‰ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”»åƒã‚’ç”Ÿæˆã—ã¦ä¿å­˜ã™ã‚‹"""
    if not font_path:
        print("\nâš ï¸  ã‚¨ãƒ©ãƒ¼: --font ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        return

    print("\nãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”»åƒã‚’ç”Ÿæˆä¸­...")
    
    try:
        wordcloud = WordCloud(
            width=800,
            height=400,
            background_color='white',
            font_path=font_path,
            max_words=150,
            colormap='viridis'
        ).generate_from_frequencies(counter)

        output_filename = "blog_wordcloud.png"
        wordcloud.to_file(output_filename)
        print(f"\nğŸ‰ å®Œæˆï¼ '{output_filename}' ã¨ã„ã†åå‰ã§ç”»åƒã‚’ä¿å­˜ã—ã¾ã—ãŸï¼")

    except Exception as e:
         print(f"ã‚¨ãƒ©ãƒ¼: ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚: {e}")

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Bloggerã®å˜èªé »åº¦ã‚’åˆ†æã—ã€ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆã—ã¾ã™ã€‚')
    parser.add_argument('--url', type=str, required=True, help='åˆ†æã—ãŸã„Bloggerã®URL')
    parser.add_argument('--days', type=int, default=30, help='åˆ†æå¯¾è±¡ã¨ã™ã‚‹æ—¥æ•°')
    parser.add_argument('--font', type=str, help='(é‡è¦)æ—¥æœ¬èªè¡¨ç¤ºã®ãŸã‚ã®ãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
    
    args = parser.parse_args()
    
    word_counter = analyze_blog(args.url, args.days)
    
    if word_counter:
        print("\n--- âœ…æœ€çµ‚çµæœ: å˜èªã®é »åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚° TOP 50 ---")
        for word, count in word_counter.most_common(50):
            print(f"{word}: {count}å›")
        
        create_word_cloud(word_counter, args.font)
