{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blogger Word Cloud Generator ğŸ“â˜ï¸\n",
    "\n",
    "ãƒ–ãƒ­ã‚°ã®URLã¨åˆ†ææœŸé–“ã‚’å…¥åŠ›ã—ã¦ã€ä¸‹ã®â–¶ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "**æ³¨:** é™¤å¤–ã—ãŸã„å˜èªã¯ã€GitHubãƒªãƒã‚¸ãƒˆãƒªã«ã‚ã‚‹ `stopwords.txt` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç·¨é›†ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "forms": {
      " à¦¬à¦¿à¦¬à¦¿à¦§ ": false,
      "à¦¸à¦¾à¦‡à¦œ ": 1,
      "à¦¸à¦®à¦¯à¦¼à§‡à¦° à¦¬à¦¿à¦¨à§à¦¯à¦¾à¦¸": false,
      "ì…ë ¥ë€": true
     }
    }
   },
   "outputs": [],
   "source": [
    "#@title â—† è¨­å®šã¨å®Ÿè¡Œ â—†\n",
    "#@markdown --- \n",
    "#@markdown ### 1. ã‚ãªãŸã®ãƒ–ãƒ­ã‚°ã¨ãƒªãƒã‚¸ãƒˆãƒªã®æƒ…å ±ã‚’å…¥åŠ›\n",
    "blog_url = \"https://amufaamo.blogspot.com/\" #@param {type:\"string\"}\n",
    "#@markdown GitHubãƒªãƒã‚¸ãƒˆãƒªã®URL (`https://github.com/ãƒ¦ãƒ¼ã‚¶ãƒ¼å/ãƒªãƒã‚¸ãƒˆãƒªå`)\n",
    "github_repo_url = \"https://github.com/amufaamo/blog-to-tagcloud\" #@param {type:\"string\"}\n",
    "days_to_analyze = 30 #@param {type:\"slider\", min:7, max:1000, step:1}\n",
    "\n",
    "#@markdown --- \n",
    "#@markdown **è¨­å®šãŒçµ‚ã‚ã£ãŸã‚‰ã€ã“ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼ˆå·¦ã®â–¶ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ï¼‰**\n",
    "import os\n",
    "\n",
    "print(\"ç’°å¢ƒã®æº–å‚™ä¸­ã§ã™...\")\n",
    "print(\"1. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã„ã¾ã™ã€‚\")\n",
    "!pip install requests beautifulsoup4 janome wordcloud matplotlib > /dev/null 2>&1\n",
    "\n",
    "print(\"2. æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™ã€‚\")\n",
    "font_path = '/content/NotoSansCJKjp-Regular.otf'\n",
    "if not os.path.exists(font_path):\n",
    "    !wget -q -O {font_path} https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/Japanese/NotoSansCJKjp-Regular.otf\n",
    "\n",
    "print(\"3. GitHubã‹ã‚‰é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ(stopwords.txt)ã‚’å–å¾—ã—ã¦ã„ã¾ã™ã€‚\")\n",
    "# GitHubã®URLã‚’RAWã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®URLã«å¤‰æ›\n",
    "stopwords_url = github_repo_url.replace('github.com', 'raw.githubusercontent.com') + '/main/stopwords.txt'\n",
    "!wget -q -O /content/stopwords.txt {stopwords_url}\n",
    "\n",
    "print(\"æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼\\n\")\n",
    "\n",
    "# --- ä»¥ä¸‹ã€åˆ†æç”¨ã®ã‚³ãƒ¼ãƒ‰ ---\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from janome.tokenizer import Tokenizer\n",
    "from collections import Counter\n",
    "import re\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_stopwords(filepath='/content/stopwords.txt'):\n",
    "    \"\"\"ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿ã€ã‚»ãƒƒãƒˆã¨ã—ã¦è¿”ã™\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        return set()\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        stopwords = {line.strip() for line in f if line.strip()}\n",
    "    print(f\"{len(stopwords)}å€‹ã®é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚\")\n",
    "    return stopwords\n",
    "\n",
    "def analyze_blog(base_url, days_to_check, stopwords):\n",
    "    all_text = \"\"\n",
    "    jst = timezone(timedelta(hours=9))\n",
    "    start_date = datetime.now(jst) - timedelta(days=days_to_check)\n",
    "    current_url = base_url\n",
    "    page_num = 1\n",
    "    print(f\"ãƒ–ãƒ­ã‚°ã®åˆ†æã‚’é–‹å§‹ã—ã¾ã™ã€‚å¯¾è±¡æœŸé–“: {days_to_check}æ—¥é–“\")\n",
    "    while current_url:\n",
    "        print(f\"ãƒšãƒ¼ã‚¸ {page_num} ã‚’å–å¾—ä¸­...\")\n",
    "        try:\n",
    "            response = requests.get(current_url)\n",
    "            response.encoding = 'utf-8'\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        except:\n",
    "            print(\"ã‚¨ãƒ©ãƒ¼: ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "            break\n",
    "        posts = soup.find_all('article', class_='post-outer-container')\n",
    "        if not posts and page_num == 1: return None\n",
    "        page_contains_valid_posts = False\n",
    "        for post in posts:\n",
    "            time_tag = post.find('time', class_='published')\n",
    "            body_tag = post.find('div', class_='post-body')\n",
    "            if not time_tag or not body_tag: continue\n",
    "            post_date = datetime.fromisoformat(time_tag['datetime'])\n",
    "            if post_date >= start_date:\n",
    "                page_contains_valid_posts = True\n",
    "                all_text += body_tag.get_text() + \"\\n\"\n",
    "        if not page_contains_valid_posts and page_num > 1: break\n",
    "        older_posts_link = soup.find('a', class_='blog-pager-older-link')\n",
    "        if older_posts_link and older_posts_link.has_attr('href'):\n",
    "            current_url = older_posts_link['href']\n",
    "            page_num += 1\n",
    "        else:\n",
    "            current_url = None\n",
    "    if not all_text: return None\n",
    "    print(\"\\nãƒ†ã‚­ã‚¹ãƒˆã®è§£æä¸­...\")\n",
    "    t = Tokenizer()\n",
    "    words = [token.surface for token in t.tokenize(all_text) \n",
    "             if token.surface not in stopwords and \n",
    "             token.part_of_speech.startswith(('åè©', 'å‹•è©', 'å½¢å®¹è©')) and \n",
    "             len(token.surface) > 1 and not re.match(r'^[0-9a-zA-Z]+$', token.surface)]\n",
    "    return Counter(words)\n",
    "\n",
    "# ãƒ¡ã‚¤ãƒ³å‡¦ç†\n",
    "custom_stopwords = load_stopwords()\n",
    "word_counter = analyze_blog(blog_url, days_to_analyze, custom_stopwords)\n",
    "\n",
    "if word_counter:\n",
    "    print(\"\\n--- âœ…å˜èªã®é »åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚° TOP 50 ---\")\n",
    "    for word, count in word_counter.most_common(50):\n",
    "        print(f\"{word}: {count}å›\")\n",
    "    \n",
    "    print(\"\\nãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”»åƒã‚’ç”Ÿæˆä¸­...\")\n",
    "    if os.path.exists(font_path) and os.path.getsize(font_path) > 1000000:\n",
    "      try:\n",
    "        wordcloud = WordCloud(\n",
    "            width=1200, height=600, background_color='white',\n",
    "            font_path=font_path, max_words=150, colormap='viridis'\n",
    "        ).generate_from_frequencies(word_counter)\n",
    "        \n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "      except OSError as e:\n",
    "        print(f\"\\nã‚¨ãƒ©ãƒ¼: ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\")\n",
    "        print(f\"ã‚¨ãƒ©ãƒ¼è©³ç´°: {e}\")\n",
    "    else:\n",
    "        print(f\"ã‚¨ãƒ©ãƒ¼ï¼šãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ããƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ã‚»ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã‹ã€ã€Œãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®å†èµ·å‹•ã€ã‚’è©¦ã—ã¦ãã ã•ã„ã€‚\")\n",
    "else:\n",
    "    print(\"ã‚¨ãƒ©ãƒ¼: åˆ†æå¯¾è±¡ã®è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚URLã‚„æœŸé–“ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
