{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blogger Word Cloud Generator ğŸ“â˜ï¸\n",
    "\n",
    "ãƒ–ãƒ­ã‚°ã®URLã¨åˆ†ææœŸé–“ã‚’å…¥åŠ›ã—ã¦ã€ä¸‹ã®â–¶ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã™ã‚‹ã¨ã€å˜èªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã¨ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”»åƒãŒã“ã®ä¸‹ã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "forms": {
      " à¦¬à¦¿à¦¬à¦¿à¦§ ": false,
      "à¦¸à¦¾à¦‡à¦œ ": 1,
      "à¦¸à¦®à¦¯à¦¼à§‡à¦° à¦¬à¦¿à¦¨à§à¦¯à¦¾à¦¸": false,
      "ì…ë ¥ë€": true
     }
    }
   },
   "outputs": [],
   "source": [
    "#@title â—† è¨­å®šã¨å®Ÿè¡Œ â—†\n",
    "#@markdown --- \n",
    "#@markdown ### åˆ†æã—ãŸã„ãƒ–ãƒ­ã‚°ã®æƒ…å ±ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„\n",
    "blog_url = \"https://amufaamo.blogspot.com/\" #@param {type:\"string\"}\n",
    "days_to_analyze = 30 #@param {type:\"slider\", min:7, max:1000, step:1}\n",
    "\n",
    "#@markdown --- \n",
    "#@markdown **è¨­å®šãŒçµ‚ã‚ã£ãŸã‚‰ã€ã“ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼ˆå·¦ã®â–¶ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ï¼‰**\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"ç’°å¢ƒã®æº–å‚™ä¸­ã§ã™...\")\n",
    "print(\"1. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã„ã¾ã™ã€‚\")\n",
    "!pip install requests beautifulsoup4 janome wordcloud matplotlib > /dev/null 2>&1\n",
    "\n",
    "print(\"2. æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™ã€‚\")\n",
    "font_path = '/content/NotoSansJP-Regular.otf'\n",
    "# ãƒ•ã‚©ãƒ³ãƒˆãŒæ—¢ã«å­˜åœ¨ã—ãªã„ã‹ã€ã‚µã‚¤ã‚ºãŒå°ã•ã™ãã‚‹ï¼ˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—ï¼‰å ´åˆã«ã®ã¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
    "if not os.path.exists(font_path) or os.path.getsize(font_path) < 1024*1024: # 1MBã‚ˆã‚Šå°ã•ã„å ´åˆã¯å¤±æ•—ã¨è¦‹ãªã™\n",
    "    !wget -q -O {font_path} https://github.com/google/fonts/raw/main/ofl/notosansjp/NotoSansJP-Regular.otf\n",
    "    print(\"ãƒ•ã‚©ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚\")\n",
    "else:\n",
    "    print(\"ãƒ•ã‚©ãƒ³ãƒˆã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚\")\n",
    "\n",
    "print(\"æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼\\n\")\n",
    "\n",
    "# --- ä»¥ä¸‹ã€åˆ†æç”¨ã®ã‚³ãƒ¼ãƒ‰ ---\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from janome.tokenizer import Tokenizer\n",
    "from collections import Counter\n",
    "import re\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CUSTOM_STOP_WORDS = {\n",
    "    'ã™ã‚‹', 'ã„ã‚‹', 'ã‚ã‚‹', 'ãªã„', 'æ€ã†', 'ã“ã¨', 'ã‚‚ã®',\n",
    "    'ãªã‚‹', 'ã‚ˆã†', 'ã¿ãŸã„', 'ã„ã†', 'ã“ã‚Œ', 'ãã‚Œ', 'ã„ã„',\n",
    "    'ã•ã‚“', 'ã¡ã‚ƒã‚“', 'ãã‚“', 'ã§ã™', 'ã¾ã™', 'ã®ã§',\n",
    "    'ã‹ã‚‰', 'ã‘ã©', 'ãŸã ', 'http', 'https', 'com', 'jp'\n",
    "}\n",
    "\n",
    "def analyze_blog(base_url, days_to_check):\n",
    "    all_text = \"\"\n",
    "    jst = timezone(timedelta(hours=9))\n",
    "    start_date = datetime.now(jst) - timedelta(days=days_to_check)\n",
    "    current_url = base_url\n",
    "    page_num = 1\n",
    "    print(f\"ãƒ–ãƒ­ã‚°ã®åˆ†æã‚’é–‹å§‹ã—ã¾ã™ã€‚å¯¾è±¡æœŸé–“: {days_to_check}æ—¥é–“\")\n",
    "    while current_url:\n",
    "        print(f\"ãƒšãƒ¼ã‚¸ {page_num} ã‚’å–å¾—ä¸­...\")\n",
    "        try:\n",
    "            response = requests.get(current_url)\n",
    "            response.encoding = 'utf-8'\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        except:\n",
    "            print(\"ã‚¨ãƒ©ãƒ¼: ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "            break\n",
    "        posts = soup.find_all('article', class_='post-outer-container')\n",
    "        if not posts and page_num == 1: return None\n",
    "        page_contains_valid_posts = False\n",
    "        for post in posts:\n",
    "            time_tag = post.find('time', class_='published')\n",
    "            body_tag = post.find('div', class_='post-body')\n",
    "            if not time_tag or not body_tag: continue\n",
    "            post_date = datetime.fromisoformat(time_tag['datetime'])\n",
    "            if post_date >= start_date:\n",
    "                page_contains_valid_posts = True\n",
    "                all_text += body_tag.get_text() + \"\\n\"\n",
    "        if not page_contains_valid_posts and page_num > 1: break\n",
    "        older_posts_link = soup.find('a', class_='blog-pager-older-link')\n",
    "        if older_posts_link and older_posts_link.has_attr('href'):\n",
    "            current_url = older_posts_link['href']\n",
    "            page_num += 1\n",
    "        else:\n",
    "            current_url = None\n",
    "    if not all_text: return None\n",
    "    print(\"\\nãƒ†ã‚­ã‚¹ãƒˆã®è§£æä¸­...\")\n",
    "    t = Tokenizer()\n",
    "    words = [token.surface for token in t.tokenize(all_text) \n",
    "             if token.surface not in CUSTOM_STOP_WORDS and \n",
    "             token.part_of_speech.startswith(('åè©', 'å‹•è©', 'å½¢å®¹è©')) and \n",
    "             len(token.surface) > 1 and not re.match(r'^[0-9a-zA-Z]+$', token.surface)]\n",
    "    return Counter(words)\n",
    "\n",
    "word_counter = analyze_blog(blog_url, days_to_analyze)\n",
    "\n",
    "if word_counter:\n",
    "    print(\"\\n--- âœ…å˜èªã®é »åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚° TOP 50 ---\")\n",
    "    for word, count in word_counter.most_common(50):\n",
    "        print(f\"{word}: {count}å›\")\n",
    "    \n",
    "    print(\"\\nãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”»åƒã‚’ç”Ÿæˆä¸­...\")\n",
    "    if os.path.exists(font_path) and os.path.getsize(font_path) > 1024*1024:\n",
    "      wordcloud = WordCloud(\n",
    "          width=1200, height=600, background_color='white',\n",
    "          font_path=font_path, max_words=150, colormap='viridis'\n",
    "      ).generate_from_frequencies(word_counter)\n",
    "      \n",
    "      plt.figure(figsize=(15, 8))\n",
    "      plt.imshow(wordcloud, interpolation='bilinear')\n",
    "      plt.axis(\"off\")\n",
    "      plt.show()\n",
    "    else:\n",
    "        print(f\"ã‚¨ãƒ©ãƒ¼ï¼šãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ãŸã‚ˆã†ã§ã™ã€‚ã‚‚ã†ä¸€åº¦ã‚»ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã‹ã€ã€Œãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®å†èµ·å‹•ã€ã‚’è©¦ã—ã¦ãã ã•ã„ã€‚\")\n",
    "else:\n",
    "    print(\"ã‚¨ãƒ©ãƒ¼: åˆ†æå¯¾è±¡ã®è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚URLã‚„æœŸé–“ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
