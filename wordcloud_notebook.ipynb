{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Blog Word Cloud Generator ğŸ“â˜ï¸\n",
    "\n",
    "**ä½¿ã„æ–¹:**\n",
    "1. **ã‚¹ãƒ†ãƒƒãƒ—1**ã§ãƒ–ãƒ­ã‚°ã®URLã¨æœŸé–“ã‚’è¨­å®šã—ã¾ã™ã€‚\n",
    "2. **ã‚¹ãƒ†ãƒƒãƒ—2**ã‚’å®Ÿè¡Œã—ã¦ã€ãƒ–ãƒ­ã‚°ã®å…¨ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã—ã¾ã™ï¼ˆã“ã®å‡¦ç†ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰ã€‚\n",
    "3. **ã‚¹ãƒ†ãƒƒãƒ—3**ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€æœ€åˆã®åˆ†æçµæœãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚çµæœã‚’è¦‹ãªãŒã‚‰ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹å†…ã®é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ã‚’ç·¨é›†ã—ã€ä½•åº¦ã§ã‚‚ã‚¹ãƒ†ãƒƒãƒ—3ã‚’å†å®Ÿè¡Œã—ã¦çµæœã‚’èª¿æ•´ã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ã‚¹ãƒ†ãƒƒãƒ—1ï¼šåˆ†æå¯¾è±¡ã®è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "forms": {
      " à¦¬à¦¿à¦¬à¦¿à¦§ ": false,
      "à¦¸à¦¾à¦‡à¦œ ": 1,
      "à¦¸à¦®à¦¯à¦¼à§‡à¦° à¦¬à¦¿à¦¨à§à¦¯à¦¾à¦¸": false,
      "ì…ë ¥ë€": true
     }
    }
   },
   "outputs": [],
   "source": [
    "#@title â—† ãƒ–ãƒ­ã‚°æƒ…å ±ã¨æœŸé–“ã‚’å…¥åŠ› â—†\n",
    "#@markdown --- \n",
    "#@markdown **åˆ†æã—ãŸã„ãƒ–ãƒ­ã‚°ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„**\n",
    "blog_url = \"\" #@param {type:\"string\"}\n",
    "#@markdown \n",
    "#@markdown --- \n",
    "#@markdown **åŸºæº–æ—¥ï¼ˆã“ã®æ—¥ã¾ã§ï¼‰ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚ç©ºæ¬„ã®å ´åˆã¯ä»Šæ—¥ã«ãªã‚Šã¾ã™ã€‚**\n",
    "base_date_str = \"\" #@param {type:\"date\"}\n",
    "#@markdown **åŸºæº–æ—¥ã‹ã‚‰ä½•æ—¥é¡ã£ã¦åˆ†æã™ã‚‹ã‹æŒ‡å®šã—ã¦ãã ã•ã„ã€‚**\n",
    "days_to_go_back = 30 #@param {type:\"slider\", min:1, max:1000, step:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ã‚¹ãƒ†ãƒƒãƒ—2ï¼šãƒ–ãƒ­ã‚°è¨˜äº‹ã®å–å¾—ï¼ˆæ™‚é–“ã®ã‹ã‹ã‚‹å‡¦ç†ï¼‰\n",
    "\n",
    "è¨­å®šãŒçµ‚ã‚ã£ãŸã‚‰ã€ä¸‹ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãƒ–ãƒ­ã‚°ã®æƒ…å ±ã‚’å–å¾—ã—ã¾ã™ã€‚ã“ã®å‡¦ç†ã¯ä¸€åº¦ã ã‘å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title â–¼ è¨˜äº‹ã®å–å¾—ã‚’å®Ÿè¡Œ\n",
    "import os\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "\n",
    "print(\"ç’°å¢ƒã®æº–å‚™ä¸­ã§ã™...\")\n",
    "!pip install requests beautifulsoup4 janome wordcloud matplotlib > /dev/null 2>&1\n",
    "font_path = '/content/NotoSansCJKjp-Regular.otf'\n",
    "if not os.path.exists(font_path) or os.path.getsize(font_path) < 1024*1024:\n",
    "    !wget -q -O {font_path} https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/Japanese/NotoSansCJKjp-Regular.otf\n",
    "github_repo_url = \"https://github.com/amufaamo/blog-to-tagcloud\"\n",
    "stopwords_path = '/content/stopwords.txt'\n",
    "stopwords_url = github_repo_url.replace('github.com', 'raw.githubusercontent.com') + '/main/stopwords.txt'\n",
    "!wget -q -O {stopwords_path} {stopwords_url}\n",
    "print(\"æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼\\n\")\n",
    "\n",
    "PLATFORM_CONFIGS = {\n",
    "    'blogger': {'post_container': 'article.post-outer-container','permalink': 'h3.post-title a','date': 'time.published','date_attribute': 'datetime','content_body': 'div.post-body','pagination': 'a.blog-pager-older-link'},\n",
    "    'hatenablog': {'post_container': 'article.entry','permalink': 'h1.entry-title a, h2.entry-title a','date': 'time[datetime]','date_attribute': 'datetime','content_body': 'div.entry-content','pagination': 'a[rel=\"next\"]'},\n",
    "    'ameblo': {'post_container': 'li[data-unique-entry-id]','permalink': 'a[data-gtm-user-entry-title]','date': 'time','date_attribute': 'datetime','content_body': 'div[data-unique-entry-body]','pagination': 'a[data-gtm-button-name=\"è¨˜äº‹ä¸€è¦§_æ¬¡ã¸\"]'},\n",
    "    'note': {'post_container': 'div.o-cardNote','permalink': 'a.o-cardNote__link','date': 'time','date_attribute': 'datetime','content_body': 'div.note-common-styles__p','pagination': None}\n",
    "}\n",
    "\n",
    "def detect_platform(url, soup):\n",
    "    if 'hatenablog' in url or soup.select_one('link[href*=\"cdn.hatena.com\"]'): return 'hatenablog'\n",
    "    if 'ameblo.jp' in url or soup.select_one('meta[property=\"og:site_name\"][content=\"Ameba\"]'): return 'ameblo'\n",
    "    if 'note.com' in url: return 'note'\n",
    "    if 'blogspot.com' in url or soup.select_one('meta[content=\"blogger\"]'): return 'blogger'\n",
    "    return None\n",
    "\n",
    "def analyze_blog(base_url, start_date, end_date):\n",
    "    all_text = \"\"\n",
    "    current_url = base_url\n",
    "    page_num = 1\n",
    "    print(f\"ãƒ–ãƒ­ã‚°ã®åˆ†æã‚’é–‹å§‹ã—ã¾ã™ã€‚å¯¾è±¡æœŸé–“: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}\")\n",
    "    try:\n",
    "        response = requests.get(current_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        response.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    platform = detect_platform(base_url, soup)\n",
    "    if not platform: return None\n",
    "    print(f\"ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ : {platform.capitalize()} ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚\")\n",
    "    config = PLATFORM_CONFIGS[platform]\n",
    "    while current_url:\n",
    "        if page_num > 1:\n",
    "            try:\n",
    "                response = requests.get(current_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                response.encoding = 'utf-8'\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            except:\n",
    "                break\n",
    "        time.sleep(1)\n",
    "        posts = soup.select(config['post_container'])\n",
    "        if not posts: break\n",
    "        stop_crawling = False\n",
    "        for post in posts:\n",
    "            date_tag = post.select_one(config['date'])\n",
    "            link_tag = post.select_one(config['permalink'])\n",
    "            if not date_tag or not link_tag: continue\n",
    "            post_date_str = date_tag.get(config['date_attribute'])\n",
    "            if not post_date_str: continue\n",
    "            try:\n",
    "                post_date = datetime.fromisoformat(post_date_str.replace('Z', '+00:00'))\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if start_date <= post_date <= end_date:\n",
    "                post_url = urljoin(base_url, link_tag['href'])\n",
    "                try:\n",
    "                    post_res = requests.get(post_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                    post_res.encoding = 'utf-8'\n",
    "                    post_soup = BeautifulSoup(post_res.text, 'html.parser')\n",
    "                    content_body = post_soup.select_one(config['content_body'])\n",
    "                    if content_body:\n",
    "                        all_text += content_body.get_text() + \"\\n\"\n",
    "                    time.sleep(1)\n",
    "                except:\n",
    "                    pass\n",
    "            elif post_date < start_date:\n",
    "                stop_crawling = True\n",
    "                break\n",
    "        if stop_crawling:\n",
    "            break\n",
    "        if config['pagination']:\n",
    "            next_page_tag = soup.select_one(config['pagination'])\n",
    "            if next_page_tag and next_page_tag.has_attr('href'):\n",
    "                current_url = urljoin(base_url, next_page_tag['href'])\n",
    "                page_num += 1\n",
    "            else:\n",
    "                current_url = None\n",
    "        else:\n",
    "            current_url = None\n",
    "    return all_text\n",
    "\n",
    "# ãƒ¡ã‚¤ãƒ³å‡¦ç†\n",
    "if not blog_url: raise ValueError(\"ã‚¹ãƒ†ãƒƒãƒ—1ã§ãƒ–ãƒ­ã‚°ã®URLãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\")\n",
    "jst = timezone(timedelta(hours=9))\n",
    "today = datetime.now(jst)\n",
    "if base_date_str: end_date_obj = datetime.strptime(base_date_str, '%Y-%m-%d').replace(hour=23, minute=59, second=59, tzinfo=jst)\n",
    "else: end_date_obj = today\n",
    "start_date_obj = end_date_obj - timedelta(days=days_to_go_back)\n",
    "\n",
    "scraped_text = analyze_blog(blog_url, start_date_obj, end_date_obj)\n",
    "if scraped_text:\n",
    "    print(\"\\nâœ… ãƒ–ãƒ­ã‚°è¨˜äº‹ã®å–å¾—ãŒå®Œäº†ã—ã¾ã—ãŸã€‚ã‚¹ãƒ†ãƒƒãƒ—3ã«é€²ã‚“ã§åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "else:\n",
    "    print(\"\\nã‚¨ãƒ©ãƒ¼: åˆ†æå¯¾è±¡ã®è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ã‚¹ãƒ†ãƒƒãƒ—3ï¼šåˆ†æã¨å¯¾è©±çš„ãªå†åˆ†æ\n",
    "\n",
    "ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã«ã€`stopwords.txt`ã‹ã‚‰èª­ã¿è¾¼ã‚“ã é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚ãƒªã‚¹ãƒˆã‚’ç·¨é›†ã—ã¦**ã“ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œï¼ˆâ–¶ï¼‰**ã™ã‚‹ã¨ã€å³åº§ã«çµæœãŒæ›´æ–°ã•ã‚Œã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "forms": {
      " à¦¬à¦¿à¦¬à¦¿à¦§ ": false,
      "à¦¸à¦¾à¦‡à¦œ ": 1,
      "à¦¸à¦®à¦¯à¦¼à§‡à¦° à¦¬à¦¿à¦¨à§à¦¯à¦¾à¦¸": false,
      "ì…ë ¥ë€": true
     }
    }
   },
   "outputs": [],
   "source": [
    "#@title â–¼ é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ã‚’ç·¨é›†ã—ã¦åˆ†æãƒ»å†åˆ†æ\n",
    "from janome.tokenizer import Tokenizer\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ã‚¹ãƒ†ãƒƒãƒ—2ãŒå®Ÿè¡Œæ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯\n",
    "if 'scraped_text' not in globals() or not scraped_text:\n",
    "    print(\"ã‚¨ãƒ©ãƒ¼ï¼šå…ˆã«ã‚¹ãƒ†ãƒƒãƒ—2ã‚’å®Ÿè¡Œã—ã¦ã€ãƒ–ãƒ­ã‚°ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ã—ã¦ãã ã•ã„ã€‚\")\n",
    "else:\n",
    "    # --- é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã®æº–å‚™ ---\n",
    "    # åˆå›å®Ÿè¡Œæ™‚ã®ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ã€ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¨ã—ã¦åˆ©ç”¨ã—ã¾ã™ã€‚\n",
    "    # 2å›ç›®ä»¥é™ã®å®Ÿè¡Œã§ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç·¨é›†ã—ãŸå†…å®¹ãŒã“ã®ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã«ç¶­æŒã•ã‚Œã¾ã™ã€‚\n",
    "    try:\n",
    "        # ã“ã®ã‚»ãƒ«ãŒåˆã‚ã¦å®Ÿè¡Œã•ã‚Œã‚‹æ™‚ã«ã ã‘ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã‚€\n",
    "        if 'editable_stopwords' not in globals():\n",
    "            with open('/content/stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "                # editable_stopwords ã«ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ä¿å­˜\n",
    "                editable_stopwords = f.read()\n",
    "                print(\"stopwords.txtã‹ã‚‰é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚\")\n",
    "    except FileNotFoundError:\n",
    "        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆï¼ˆã‚¹ãƒ†ãƒƒãƒ—2ãŒæœªå®Ÿè¡Œãªã©ï¼‰\n",
    "        if 'editable_stopwords' not in globals():\n",
    "            editable_stopwords = \"\" # ç©ºã®çŠ¶æ…‹ã§é–‹å§‹\n",
    "            print(\"è­¦å‘Š: stopwords.txt ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "\n",
    "    #@markdown ---\n",
    "    #@markdown **â†“ã®ãƒªã‚¹ãƒˆã‚’è‡ªç”±ã«ç·¨é›†ã—ã€ã“ã®ã‚»ãƒ«ï¼ˆâ–¶ï¼‰ã‚’å†å®Ÿè¡Œã™ã‚‹ã¨ã€çµæœãŒæ›´æ–°ã•ã‚Œã¾ã™ã€‚**\n",
    "    #@markdown ï¼ˆ1è¡Œã«1å˜èªã®å½¢å¼ã§è¿½åŠ ãƒ»å‰Šé™¤ã—ã¦ãã ã•ã„ï¼‰\n",
    "    editable_stopwords = editable_stopwords #@param {type:\"string\"}\n",
    "\n",
    "    # --- ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†æ ---\n",
    "    def reanalyze_text(text, stopwords_str):\n",
    "        # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ã®æ–‡å­—åˆ—ã‚’æ”¹è¡Œã§åˆ†å‰²ã—ã€é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ã®ã‚»ãƒƒãƒˆã‚’ä½œæˆ\n",
    "        stopwords = {line.strip() for line in stopwords_str.splitlines() if line.strip()}\n",
    "        print(f\"\\n{len(stopwords)}å€‹ã®é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’å†åˆ†æä¸­...\")\n",
    "        t = Tokenizer()\n",
    "        words = [token.surface for token in t.tokenize(text)\n",
    "                 if token.surface not in stopwords and\n",
    "                 token.part_of_speech.startswith(('åè©', 'å‹•è©', 'å½¢å®¹è©')) and\n",
    "                 len(token.surface) > 1 and not re.match(r'^[0-9a-zA-Z]+$', token.surface)]\n",
    "        return Counter(words)\n",
    "\n",
    "    # --- çµæœã®è¡¨ç¤º ---\n",
    "    def show_results(counter):\n",
    "        print(\"\\n--- âœ…å˜èªã®é »åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚° TOP 50 ---\")\n",
    "        for word, count in counter.most_common(50):\n",
    "            print(f\"{word}: {count}å›\")\n",
    "\n",
    "        font_path = '/content/NotoSansCJKjp-Regular.otf'\n",
    "        # ã‚¹ãƒ†ãƒƒãƒ—2ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ã‚’ãƒã‚§ãƒƒã‚¯\n",
    "        if os.path.exists(font_path) and os.path.getsize(font_path) > 1000000:\n",
    "          print(\"\\nãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”»åƒã‚’ç”Ÿæˆä¸­...\")\n",
    "          try:\n",
    "            wordcloud = WordCloud(width=1200, height=600, background_color='white', font_path=font_path, max_words=150, colormap='viridis').generate_from_frequencies(counter)\n",
    "            plt.figure(figsize=(15, 8))\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "          except Exception as e:\n",
    "            # ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒèµ·ããŸå ´åˆ\n",
    "            print(f\"\\nã‚¨ãƒ©ãƒ¼: ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚è©³ç´°: {e}\")\n",
    "        else:\n",
    "            # ãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ\n",
    "            print(\"\\nã‚¨ãƒ©ãƒ¼ï¼šãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”Ÿæˆç”¨ã®ãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ãƒ†ãƒƒãƒ—2ã‚’å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "    # --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---\n",
    "    # ç·¨é›†ã•ã‚ŒãŸé™¤å¤–ãƒ¯ãƒ¼ãƒ‰ã‚’ä½¿ã£ã¦åˆ†æã‚’å®Ÿè¡Œ\n",
    "    word_counter = reanalyze_text(scraped_text, editable_stopwords)\n",
    "    if word_counter:\n",
    "        show_results(word_counter)\n",
    "    else:\n",
    "        print(\"\\nåˆ†æã®çµæœã€æœ‰åŠ¹ãªå˜èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’è¦‹ç›´ã—ã¦ã¿ã¦ãã ã•ã„ã€‚\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
