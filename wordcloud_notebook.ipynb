{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blogger Word Cloud Generator 📝☁️\n",
    "\n",
    "ブログのURLと分析期間を入力して、下の▶ボタンを押してください。\n",
    "\n",
    "すべての処理が完了すると、単語ランキングとワードクラウド画像がこの下に表示されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "forms": {
      " বিবিধ ": false,
      "সাইজ ": 1,
      "সময়ের বিন্যাস": false,
      "입력란": true
     }
    }
   },
   "outputs": [],
   "source": [
    "#@title ◆ 設定と実行 ◆\n",
    "#@markdown --- \n",
    "#@markdown ### 分析したいブログの情報を入力してください\n",
    "blog_url = \"https://amufaamo.blogspot.com/\" #@param {type:\"string\"}\n",
    "days_to_analyze = 30 #@param {type:\"slider\", min:7, max:1000, step:1}\n",
    "\n",
    "#@markdown --- \n",
    "#@markdown **設定が終わったら、このセルを実行してください（左の▶ボタンをクリック）**\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"環境の準備中です...\")\n",
    "print(\"1. 必要なライブラリをインストールしています。\")\n",
    "!pip install requests beautifulsoup4 janome wordcloud matplotlib > /dev/null 2>&1\n",
    "\n",
    "print(\"2. 日本語フォントをダウンロードしています。\")\n",
    "font_path = '/content/NotoSansJP-Regular.otf'\n",
    "# フォントが既に存在しないか、サイズが小さすぎる（ダウンロード失敗）場合にのみダウンロード\n",
    "if not os.path.exists(font_path) or os.path.getsize(font_path) < 1024*1024: # 1MBより小さい場合は失敗と見なす\n",
    "    !wget -q -O {font_path} https://github.com/google/fonts/raw/main/ofl/notosansjp/NotoSansJP-Regular.otf\n",
    "    print(\"フォントをダウンロードしました。\")\n",
    "else:\n",
    "    print(\"フォントは既に存在します。\")\n",
    "\n",
    "print(\"準備が完了しました！\\n\")\n",
    "\n",
    "# --- 以下、分析用のコード ---\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from janome.tokenizer import Tokenizer\n",
    "from collections import Counter\n",
    "import re\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "CUSTOM_STOP_WORDS = {\n",
    "    'する', 'いる', 'ある', 'ない', '思う', 'こと', 'もの',\n",
    "    'なる', 'よう', 'みたい', 'いう', 'これ', 'それ', 'いい',\n",
    "    'さん', 'ちゃん', 'くん', 'です', 'ます', 'ので',\n",
    "    'から', 'けど', 'ただ', 'http', 'https', 'com', 'jp'\n",
    "}\n",
    "\n",
    "def analyze_blog(base_url, days_to_check):\n",
    "    all_text = \"\"\n",
    "    jst = timezone(timedelta(hours=9))\n",
    "    start_date = datetime.now(jst) - timedelta(days=days_to_check)\n",
    "    current_url = base_url\n",
    "    page_num = 1\n",
    "    print(f\"ブログの分析を開始します。対象期間: {days_to_check}日間\")\n",
    "    while current_url:\n",
    "        print(f\"ページ {page_num} を取得中...\")\n",
    "        try:\n",
    "            response = requests.get(current_url)\n",
    "            response.encoding = 'utf-8'\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        except:\n",
    "            print(\"エラー: ページにアクセスできませんでした。\")\n",
    "            break\n",
    "        posts = soup.find_all('article', class_='post-outer-container')\n",
    "        if not posts and page_num == 1: return None\n",
    "        page_contains_valid_posts = False\n",
    "        for post in posts:\n",
    "            time_tag = post.find('time', class_='published')\n",
    "            body_tag = post.find('div', class_='post-body')\n",
    "            if not time_tag or not body_tag: continue\n",
    "            post_date = datetime.fromisoformat(time_tag['datetime'])\n",
    "            if post_date >= start_date:\n",
    "                page_contains_valid_posts = True\n",
    "                all_text += body_tag.get_text() + \"\\n\"\n",
    "        if not page_contains_valid_posts and page_num > 1: break\n",
    "        older_posts_link = soup.find('a', class_='blog-pager-older-link')\n",
    "        if older_posts_link and older_posts_link.has_attr('href'):\n",
    "            current_url = older_posts_link['href']\n",
    "            page_num += 1\n",
    "        else:\n",
    "            current_url = None\n",
    "    if not all_text: return None\n",
    "    print(\"\\nテキストの解析中...\")\n",
    "    t = Tokenizer()\n",
    "    words = [token.surface for token in t.tokenize(all_text) \n",
    "             if token.surface not in CUSTOM_STOP_WORDS and \n",
    "             token.part_of_speech.startswith(('名詞', '動詞', '形容詞')) and \n",
    "             len(token.surface) > 1 and not re.match(r'^[0-9a-zA-Z]+$', token.surface)]\n",
    "    return Counter(words)\n",
    "\n",
    "word_counter = analyze_blog(blog_url, days_to_analyze)\n",
    "\n",
    "if word_counter:\n",
    "    print(\"\\n--- ✅単語の頻度ランキング TOP 50 ---\")\n",
    "    for word, count in word_counter.most_common(50):\n",
    "        print(f\"{word}: {count}回\")\n",
    "    \n",
    "    print(\"\\nワードクラウド画像を生成中...\")\n",
    "    if os.path.exists(font_path) and os.path.getsize(font_path) > 1024*1024:\n",
    "      wordcloud = WordCloud(\n",
    "          width=1200, height=600, background_color='white',\n",
    "          font_path=font_path, max_words=150, colormap='viridis'\n",
    "      ).generate_from_frequencies(word_counter)\n",
    "      \n",
    "      plt.figure(figsize=(15, 8))\n",
    "      plt.imshow(wordcloud, interpolation='bilinear')\n",
    "      plt.axis(\"off\")\n",
    "      plt.show()\n",
    "    else:\n",
    "        print(f\"エラー：フォントファイルのダウンロードに失敗したようです。もう一度セルを実行するか、「ランタイムの再起動」を試してください。\")\n",
    "else:\n",
    "    print(\"エラー: 分析対象の記事が見つかりませんでした。URLや期間を確認してください。\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
