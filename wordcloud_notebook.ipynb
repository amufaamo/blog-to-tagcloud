{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blogger Word Cloud Generator ğŸ“â˜ï¸\n",
    "\n",
    "åˆ†æã—ãŸã„ãƒ–ãƒ­ã‚°ã®URLã¨æœŸé–“ï¼ˆé–‹å§‹æ—¥ãƒ»çµ‚äº†æ—¥ï¼‰ã‚’å…¥åŠ›ã—ã¦ã€ä¸‹ã®â–¶ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "**æ³¨:** é™¤å¤–ã—ãŸã„å˜èªã¯ã€GitHubãƒªãƒã‚¸ãƒˆãƒªã«ã‚ã‚‹ `stopwords.txt` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥ç·¨é›†ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "forms": {
      " à¦¬à¦¿à¦¬à¦¿à¦§ ": false,
      "à¦¸à¦¾à¦‡à¦œ ": 1,
      "à¦¸à¦®à¦¯à¦¼à§‡à¦° à¦¬à¦¿à¦¨à§à¦¯à¦¾à¦¸": false,
      "ì…ë ¥ë€": true
     }
    }
   },
   "outputs": [],
   "source": [
    "#@title â—† è¨­å®šã¨å®Ÿè¡Œ â—†\n",
    "#@markdown --- \n",
    "#@markdown ### 1. åˆ†æã—ãŸã„ãƒ–ãƒ­ã‚°ã®æƒ…å ±ã‚’å…¥åŠ›\n",
    "#@markdown **åˆ†æã—ãŸã„ãƒ–ãƒ­ã‚°ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„**\n",
    "blog_url = \"\" #@param {type:\"string\"}\n",
    "#@markdown \n",
    "#@markdown --- \n",
    "#@markdown ### 2. åˆ†æã™ã‚‹æœŸé–“ã‚’æŒ‡å®š\n",
    "#@markdown **å…¥åŠ›ãŒãªã„å ´åˆã¯ã€éå»30æ—¥é–“ã®è¨˜äº‹ã‚’åˆ†æã—ã¾ã™**\n",
    "start_date_str = \"\" #@param {type:\"date\"}\n",
    "end_date_str = \"\" #@param {type:\"date\"}\n",
    "\n",
    "#@markdown --- \n",
    "#@markdown **è¨­å®šãŒçµ‚ã‚ã£ãŸã‚‰ã€ã“ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼ˆå·¦ã®â–¶ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ï¼‰**\n",
    "import os\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "if not blog_url:\n",
    "    raise ValueError(\"ãƒ–ãƒ­ã‚°ã®URLãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å…¥åŠ›ã—ã¦ã‹ã‚‰å†åº¦å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "# --- æ—¥ä»˜ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š ---\n",
    "jst = timezone(timedelta(hours=9))\n",
    "today = datetime.now(jst)\n",
    "\n",
    "if end_date_str:\n",
    "    # å…¥åŠ›ã•ã‚ŒãŸæ—¥ä»˜ã®çµ‚ã‚ã‚Šï¼ˆ23:59:59ï¼‰ã«è¨­å®š\n",
    "    end_date_obj = datetime.strptime(end_date_str, '%Y-%m-%d').replace(hour=23, minute=59, second=59, tzinfo=jst)\n",
    "else:\n",
    "    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ä»Šæ—¥\n",
    "    end_date_obj = today\n",
    "\n",
    "if start_date_str:\n",
    "    # å…¥åŠ›ã•ã‚ŒãŸæ—¥ä»˜ã®å§‹ã¾ã‚Šï¼ˆ00:00:00ï¼‰ã«è¨­å®š\n",
    "    start_date_obj = datetime.strptime(start_date_str, '%Y-%m-%d').replace(tzinfo=jst)\n",
    "else:\n",
    "    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯çµ‚äº†æ—¥ã‹ã‚‰30æ—¥å‰\n",
    "    start_date_obj = end_date_obj - timedelta(days=30)\n",
    "\n",
    "# --- ç’°å¢ƒæº–å‚™ ---\n",
    "github_repo_url = \"https://github.com/amufaamo/blog-to-tagcloud\"\n",
    "\n",
    "print(\"ç’°å¢ƒã®æº–å‚™ä¸­ã§ã™...\")\n",
    "print(\"1. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã„ã¾ã™ã€‚\")\n",
    "!pip install requests beautifulsoup4 janome wordcloud matplotlib > /dev/null 2>&1\n",
    "\n",
    "print(\"2. æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™ã€‚\")\n",
    "font_path = '/content/NotoSansCJKjp-Regular.otf'\n",
    "if not os.path.exists(font_path) or os.path.getsize(font_path) < 1024*1024: \n",
    "    !wget -q -O {font_path} https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/Japanese/NotoSansCJKjp-Regular.otf\n",
    "\n",
    "print(\"3. GitHubã‹ã‚‰é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ(stopwords.txt)ã‚’å–å¾—ã—ã¦ã„ã¾ã™ã€‚\")\n",
    "stopwords_url = github_repo_url.replace('github.com', 'raw.githubusercontent.com') + '/main/stopwords.txt'\n",
    "!wget -q -O /content/stopwords.txt {stopwords_url}\n",
    "\n",
    "print(\"æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼\\n\")\n",
    "\n",
    "# --- ä»¥ä¸‹ã€åˆ†æç”¨ã®ã‚³ãƒ¼ãƒ‰ ---\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from janome.tokenizer import Tokenizer\n",
    "from collections import Counter\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_stopwords(filepath='/content/stopwords.txt'):\n",
    "    if not os.path.exists(filepath):\n",
    "        return set()\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        stopwords = {line.strip() for line in f if line.strip()}\n",
    "    print(f\"{len(stopwords)}å€‹ã®é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚\")\n",
    "    return stopwords\n",
    "\n",
    "def analyze_blog(base_url, start_date, end_date, stopwords):\n",
    "    all_text = \"\"\n",
    "    current_url = base_url\n",
    "    page_num = 1\n",
    "    print(f\"ãƒ–ãƒ­ã‚°ã®åˆ†æã‚’é–‹å§‹ã—ã¾ã™ã€‚\")\n",
    "    print(f\"å¯¾è±¡æœŸé–“: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    while current_url:\n",
    "        print(f\"ãƒšãƒ¼ã‚¸ {page_num} ã‚’å–å¾—ä¸­...\")\n",
    "        try:\n",
    "            response = requests.get(current_url)\n",
    "            response.encoding = 'utf-8'\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        except:\n",
    "            print(\"ã‚¨ãƒ©ãƒ¼: ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "            break\n",
    "        posts = soup.find_all('article', class_='post-outer-container')\n",
    "        if not posts and page_num == 1: return None\n",
    "        \n",
    "        page_contains_valid_posts = False\n",
    "        stop_crawling = False\n",
    "        for post in posts:\n",
    "            time_tag = post.find('time', class_='published')\n",
    "            body_tag = post.find('div', class_='post-body')\n",
    "            if not time_tag or not body_tag: continue\n",
    "            \n",
    "            post_date = datetime.fromisoformat(time_tag['datetime'])\n",
    "            # è¨˜äº‹ã®æ—¥ä»˜ãŒæœŸé–“å†…ã‹ãƒã‚§ãƒƒã‚¯\n",
    "            if start_date <= post_date <= end_date:\n",
    "                page_contains_valid_posts = True\n",
    "                all_text += body_tag.get_text() + \"\\n\"\n",
    "            # è¨˜äº‹ãŒé–‹å§‹æ—¥ã‚ˆã‚Šå¤ã‘ã‚Œã°ã€ã“ã‚Œä»¥ä¸Šã•ã‹ã®ã¼ã‚‹ã®ã‚’ã‚„ã‚ã‚‹\n",
    "            elif post_date < start_date:\n",
    "                stop_crawling = True\n",
    "                break\n",
    "        \n",
    "        if stop_crawling:\n",
    "            print(\"å¯¾è±¡æœŸé–“å¤–ã®è¨˜äº‹ã«åˆ°é”ã—ãŸãŸã‚ã€ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’çµ‚äº†ã—ã¾ã™ã€‚\")\n",
    "            break\n",
    "\n",
    "        older_posts_link = soup.find('a', class_='blog-pager-older-link')\n",
    "        if older_posts_link and older_posts_link.has_attr('href'):\n",
    "            current_url = older_posts_link['href']\n",
    "            page_num += 1\n",
    "        else:\n",
    "            current_url = None\n",
    "            \n",
    "    if not all_text: return None\n",
    "    print(\"\\nãƒ†ã‚­ã‚¹ãƒˆã®è§£æä¸­...\")\n",
    "    t = Tokenizer()\n",
    "    words = [token.surface for token in t.tokenize(all_text) \n",
    "             if token.surface not in stopwords and \n",
    "             token.part_of_speech.startswith(('åè©', 'å‹•è©', 'å½¢å®¹è©')) and \n",
    "             len(token.surface) > 1 and not re.match(r'^[0-9a-zA-Z]+$', token.surface)]\n",
    "    return Counter(words)\n",
    "\n",
    "custom_stopwords = load_stopwords()\n",
    "word_counter = analyze_blog(blog_url, start_date_obj, end_date_obj, custom_stopwords)\n",
    "\n",
    "if word_counter:\n",
    "    print(\"\\n--- âœ…å˜èªã®é »åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚° TOP 50 ---\")\n",
    "    for word, count in word_counter.most_common(50):\n",
    "        print(f\"{word}: {count}å›\")\n",
    "    \n",
    "    print(\"\\nãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”»åƒã‚’ç”Ÿæˆä¸­...\")\n",
    "    if os.path.exists(font_path) and os.path.getsize(font_path) > 1000000:\n",
    "      try:\n",
    "        wordcloud = WordCloud(\n",
    "            width=1200, height=600, background_color='white',\n",
    "            font_path=font_path, max_words=150, colormap='viridis'\n",
    "        ).generate_from_frequencies(word_counter)\n",
    "        \n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "      except OSError as e:\n",
    "        print(f\"\\nã‚¨ãƒ©ãƒ¼: ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚\")\n",
    "    else:\n",
    "        print(f\"ã‚¨ãƒ©ãƒ¼ï¼šãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ããƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã€Œãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®å†èµ·å‹•ã€ã‚’è©¦ã—ã¦ãã ã•ã„ã€‚\")\n",
    "else:\n",
    "    print(\"ã‚¨ãƒ©ãƒ¼: åˆ†æå¯¾è±¡ã®è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚URLã‚„æœŸé–“ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
