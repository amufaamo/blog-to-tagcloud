{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Blog Word Cloud Generator 📝☁️\n",
    "\n",
    "Blogger、はてなブログ(カスタムドメイン対応)、note、アメーバブログなど、さまざまなブログの記事を分析してワードクラウドを生成します。\n",
    "\n",
    "分析したいブログのURL、基準日、遡る日数を入力して、下の▶ボタンを押してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "forms": {
      " বিবিধ ": false,
      "সাইজ ": 1,
      "সময়ের বিন্যাস": false,
      "입력란": true
     }
    }
   },
   "outputs": [],
   "source": [
    "#@title ◆ 設定と実行 ◆\n",
    "#@markdown --- \n",
    "#@markdown ### 1. 分析したいブログの情報を入力\n",
    "blog_url = \"https://www.tyoshiki.com/\" #@param {type:\"string\"}\n",
    "#@markdown \n",
    "#@markdown --- \n",
    "#@markdown ### 2. 分析する期間を指定\n",
    "base_date_str = \"\" #@param {type:\"date\"}\n",
    "days_to_go_back = 30 #@param {type:\"slider\", min:1, max:1000, step:1}\n",
    "\n",
    "#@markdown --- \n",
    "#@markdown **設定が終わったら、このセルを実行してください（左の▶ボタンをクリック）**\n",
    "import os\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "if not blog_url:\n",
    "    raise ValueError(\"ブログのURLが入力されていません。入力してから再度実行してください。\")\n",
    "\n",
    "jst = timezone(timedelta(hours=9))\n",
    "today = datetime.now(jst)\n",
    "if base_date_str:\n",
    "    end_date_obj = datetime.strptime(base_date_str, '%Y-%m-%d').replace(hour=23, minute=59, second=59, tzinfo=jst)\n",
    "else:\n",
    "    end_date_obj = today\n",
    "start_date_obj = end_date_obj - timedelta(days=days_to_go_back)\n",
    "\n",
    "# --- 環境準備 ---\n",
    "github_repo_url = \"https://github.com/amufaamo/blog-to-tagcloud\"\n",
    "print(\"環境の準備中です...\")\n",
    "!pip install requests beautifulsoup4 janome wordcloud matplotlib > /dev/null 2>&1\n",
    "font_path = '/content/NotoSansCJKjp-Regular.otf'\n",
    "if not os.path.exists(font_path) or os.path.getsize(font_path) < 1024*1024:\n",
    "    !wget -q -O {font_path} https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/Japanese/NotoSansCJKjp-Regular.otf\n",
    "stopwords_url = github_repo_url.replace('github.com', 'raw.githubusercontent.com') + '/main/stopwords.txt'\n",
    "!wget -q -O /content/stopwords.txt {stopwords_url}\n",
    "print(\"準備が完了しました！\\n\")\n",
    "\n",
    "# --- ▼▼▼ ここからが新しい分析エンジン ▼▼▼ ---\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from janome.tokenizer import Tokenizer\n",
    "from collections import Counter\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "PLATFORM_CONFIGS = {\n",
    "    'blogger': {\n",
    "        'post_container': 'article.post-outer-container',\n",
    "        'permalink': 'h3.post-title a',\n",
    "        'date': 'time.published',\n",
    "        'date_attribute': 'datetime',\n",
    "        'content_body': 'div.post-body',\n",
    "        'pagination': 'a.blog-pager-older-link',\n",
    "    },\n",
    "    'hatenablog': {\n",
    "        'post_container': 'article.entry',\n",
    "        'permalink': 'h1.entry-title a, h2.entry-title a',\n",
    "        'date': 'time[datetime]',\n",
    "        'date_attribute': 'datetime',\n",
    "        'content_body': 'div.entry-content',\n",
    "        'pagination': 'a[rel=\"next\"]',\n",
    "    },\n",
    "    'ameblo': {\n",
    "        'post_container': 'li[data-unique-entry-id]',\n",
    "        'permalink': 'a[data-gtm-user-entry-title]',\n",
    "        'date': 'time',\n",
    "        'date_attribute': 'datetime',\n",
    "        'content_body': 'div[data-unique-entry-body]',\n",
    "        'pagination': 'a[data-gtm-button-name=\"記事一覧_次へ\"]',\n",
    "    },\n",
    "    'note': {\n",
    "        'post_container': 'div.o-cardNote',\n",
    "        'permalink': 'a.o-cardNote__link',\n",
    "        'date': 'time',\n",
    "        'date_attribute': 'datetime',\n",
    "        'content_body': 'div.note-common-styles__p',\n",
    "        'pagination': None, # 無限スクロールのため\n",
    "    }\n",
    "}\n",
    "\n",
    "def detect_platform(url, soup):\n",
    "    \"\"\"URLの文字列とHTMLの中身からプラットフォームを判別する\"\"\"\n",
    "    if 'hatenablog' in url or soup.select_one('link[href*=\"cdn.hatena.com\"]'):\n",
    "        return 'hatenablog'\n",
    "    if 'ameblo.jp' in url or soup.select_one('meta[property=\"og:site_name\"][content=\"Ameba\"]'):\n",
    "        return 'ameblo'\n",
    "    if 'note.com' in url:\n",
    "        return 'note'\n",
    "    if 'blogspot.com' in url or soup.select_one('meta[content=\"blogger\"]'):\n",
    "        return 'blogger'\n",
    "    return None\n",
    "\n",
    "def load_stopwords(filepath='/content/stopwords.txt'):\n",
    "    if not os.path.exists(filepath): return set()\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        stopwords = {line.strip() for line in f if line.strip()}\n",
    "    print(f\"{len(stopwords)}個の除外ワードを読み込みました。\")\n",
    "    return stopwords\n",
    "\n",
    "def analyze_blog(base_url, start_date, end_date, stopwords):\n",
    "    all_text = \"\"\n",
    "    current_url = base_url\n",
    "    page_num = 1\n",
    "    \n",
    "    print(f\"ブログの分析を開始します。対象期間: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # 最初のページを取得してプラットフォームを判別\n",
    "    try:\n",
    "        response = requests.get(current_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        response.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    except Exception as e:\n",
    "        print(f\"エラー: 最初のページにアクセスできませんでした。URLを確認してください。\\n{e}\")\n",
    "        return None\n",
    "        \n",
    "    platform = detect_platform(base_url, soup)\n",
    "    if not platform:\n",
    "        print(\"エラー: 対応していないブログプラットフォームです。 (Blogger, はてな, note, アメブロに対応)\")\n",
    "        return None\n",
    "    print(f\"プラットフォーム: {platform.capitalize()} を検出しました。\")\n",
    "    config = PLATFORM_CONFIGS[platform]\n",
    "    \n",
    "    while current_url:\n",
    "        if page_num > 1: # 2ページ目以降はここで取得\n",
    "            print(f\"記事一覧ページ {page_num} を取得中: {current_url}\")\n",
    "            try:\n",
    "                response = requests.get(current_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                response.encoding = 'utf-8'\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            except:\n",
    "                print(\"エラー: ページにアクセスできませんでした。\")\n",
    "                break\n",
    "        \n",
    "        time.sleep(1) # サーバーに優しく\n",
    "        posts = soup.select(config['post_container'])\n",
    "        if not posts: \n",
    "            print(\"これ以上記事が見つかりませんでした。クロールを終了します。\")\n",
    "            break\n",
    "\n",
    "        stop_crawling = False\n",
    "        for post in posts:\n",
    "            date_tag = post.select_one(config['date'])\n",
    "            link_tag = post.select_one(config['permalink'])\n",
    "            if not date_tag or not link_tag: continue\n",
    "\n",
    "            post_date_str = date_tag.get(config['date_attribute'])\n",
    "            if not post_date_str: continue\n",
    "            try:\n",
    "                post_date = datetime.fromisoformat(post_date_str.replace('Z', '+00:00'))\n",
    "            except ValueError:\n",
    "                continue\n",
    "            \n",
    "            if start_date <= post_date <= end_date:\n",
    "                post_url = urljoin(base_url, link_tag['href'])\n",
    "                print(f\"  -> 期間内の記事({post_date.strftime('%Y-%m-%d')})を取得中...\")\n",
    "                try:\n",
    "                    post_res = requests.get(post_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                    post_res.encoding = 'utf-8'\n",
    "                    post_soup = BeautifulSoup(post_res.text, 'html.parser')\n",
    "                    content_body = post_soup.select_one(config['content_body'])\n",
    "                    if content_body:\n",
    "                        all_text += content_body.get_text() + \"\\n\"\n",
    "                    time.sleep(1)\n",
    "                except:\n",
    "                    print(f\"     -> 記事ページの取得に失敗しました。\")\n",
    "            elif post_date < start_date:\n",
    "                stop_crawling = True\n",
    "                break\n",
    "        \n",
    "        if stop_crawling:\n",
    "            print(\"対象期間外の記事に到達したため、クロールを終了します。\")\n",
    "            break\n",
    "        \n",
    "        if config['pagination']:\n",
    "            next_page_tag = soup.select_one(config['pagination'])\n",
    "            if next_page_tag and next_page_tag.has_attr('href'):\n",
    "                current_url = urljoin(base_url, next_page_tag['href'])\n",
    "                page_num += 1\n",
    "            else:\n",
    "                current_url = None\n",
    "        else: \n",
    "            print(f\"{platform.capitalize()}は複数ページのクロールに非対応のため、最初のページのみ分析します。\")\n",
    "            current_url = None\n",
    "\n",
    "    if not all_text: return None\n",
    "    print(\"\\nテキストの解析中...\")\n",
    "    t = Tokenizer()\n",
    "    words = [token.surface for token in t.tokenize(all_text) \n",
    "             if token.surface not in stopwords and \n",
    "             token.part_of_speech.startswith(('名詞', '動詞', '形容詞')) and \n",
    "             len(token.surface) > 1 and not re.match(r'^[0-9a-zA-Z]+$', token.surface)]\n",
    "    return Counter(words)\n",
    "\n",
    "# メイン処理\n",
    "custom_stopwords = load_stopwords()\n",
    "word_counter = analyze_blog(blog_url, start_date_obj, end_date_obj, custom_stopwords)\n",
    "\n",
    "if word_counter:\n",
    "    print(\"\\n--- ✅単語の頻度ランキング TOP 50 ---\")\n",
    "    for word, count in word_counter.most_common(50):\n",
    "        print(f\"{word}: {count}回\")\n",
    "    print(\"\\nワードクラウド画像を生成中...\")\n",
    "    if os.path.exists(font_path) and os.path.getsize(font_path) > 1000000:\n",
    "      try:\n",
    "        wordcloud = WordCloud(width=1200, height=600, background_color='white', font_path=font_path, max_words=150, colormap='viridis').generate_from_frequencies(word_counter)\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "      except OSError as e:\n",
    "        print(f\"\\nエラー: ワードクラウドの生成中にエラーが発生しました。\")\n",
    "    else:\n",
    "        print(f\"エラー：フォントファイルが正しくダウンロードできませんでした。「ランタイムの再起動」を試してください。\")\n",
    "else:\n",
    "    print(\"エラー: 分析対象の記事が見つかりませんでした。URLや期間、ブログの構造を確認してください。\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
