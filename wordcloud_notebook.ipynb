{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Blog Word Cloud Generator ğŸ“â˜ï¸\n",
    "\n",
    "Bloggerã€ã¯ã¦ãªãƒ–ãƒ­ã‚°ã€noteã€ã‚¢ãƒ¡ãƒ¼ãƒãƒ–ãƒ­ã‚°ãªã©ã€ã•ã¾ã–ã¾ãªãƒ–ãƒ­ã‚°ã®è¨˜äº‹ã‚’åˆ†æã—ã¦ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ç”Ÿæˆã—ã¾ã™ã€‚\n",
    "\n",
    "åˆ†æã—ãŸã„ãƒ–ãƒ­ã‚°ã®URLã€åŸºæº–æ—¥ã€é¡ã‚‹æ—¥æ•°ã‚’å…¥åŠ›ã—ã¦ã€ä¸‹ã®â–¶ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "forms": {
      " à¦¬à¦¿à¦¬à¦¿à¦§ ": false,
      "à¦¸à¦¾à¦‡à¦œ ": 1,
      "à¦¸à¦®à¦¯à¦¼à§‡à¦° à¦¬à¦¿à¦¨à§à¦¯à¦¾à¦¸": false,
      "ì…ë ¥ë€": true
     }
    }
   },
   "outputs": [],
   "source": [
    "#@title â—† è¨­å®šã¨å®Ÿè¡Œ â—†\n",
    "#@markdown --- \n",
    "#@markdown ### 1. åˆ†æã—ãŸã„ãƒ–ãƒ­ã‚°ã®æƒ…å ±ã‚’å…¥åŠ›\n",
    "#@markdown **åˆ†æã—ãŸã„ãƒ–ãƒ­ã‚°ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„**\n",
    "blog_url = \"\" #@param {type:\"string\"}\n",
    "#@markdown \n",
    "#@markdown --- \n",
    "#@markdown ### 2. åˆ†æã™ã‚‹æœŸé–“ã‚’æŒ‡å®š\n",
    "#@markdown **åŸºæº–æ—¥ï¼ˆã“ã®æ—¥ã¾ã§ï¼‰ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚ç©ºæ¬„ã®å ´åˆã¯ä»Šæ—¥ã«ãªã‚Šã¾ã™ã€‚**\n",
    "base_date_str = \"\" #@param {type:\"date\"}\n",
    "#@markdown **åŸºæº–æ—¥ã‹ã‚‰ä½•æ—¥é¡ã£ã¦åˆ†æã™ã‚‹ã‹æŒ‡å®šã—ã¦ãã ã•ã„ã€‚**\n",
    "days_to_go_back = 30 #@param {type:\"slider\", min:1, max:1000, step:1}\n",
    "\n",
    "#@markdown --- \n",
    "#@markdown **è¨­å®šãŒçµ‚ã‚ã£ãŸã‚‰ã€ã“ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼ˆå·¦ã®â–¶ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ï¼‰**\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from janome.tokenizer import Tokenizer\n",
    "from collections import Counter\n",
    "import re\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from urllib.parse import urljoin\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "\n",
    "if not blog_url:\n",
    "    raise ValueError(\"ãƒ–ãƒ­ã‚°ã®URLãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å…¥åŠ›ã—ã¦ã‹ã‚‰å†åº¦å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "# --- æ—¥ä»˜ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š ---\n",
    "jst = timezone(timedelta(hours=9))\n",
    "today = datetime.now(jst)\n",
    "if base_date_str:\n",
    "    end_date_obj = datetime.strptime(base_date_str, '%Y-%m-%d').replace(hour=23, minute=59, second=59, tzinfo=jst)\n",
    "else:\n",
    "    end_date_obj = today\n",
    "start_date_obj = end_date_obj - timedelta(days=days_to_go_back)\n",
    "\n",
    "# --- ç’°å¢ƒæº–å‚™ ---\n",
    "print(\"ç’°å¢ƒã®æº–å‚™ä¸­ã§ã™...\")\n",
    "print(\"1. å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ã„ã¾ã™ã€‚\")\n",
    "!pip install requests beautifulsoup4 janome wordcloud matplotlib > /dev/null 2>&1\n",
    "print(\"2. æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™ã€‚\")\n",
    "font_path = '/content/NotoSansCJKjp-Regular.otf'\n",
    "if not os.path.exists(font_path) or os.path.getsize(font_path) < 1024*1024:\n",
    "    !wget -q -O {font_path} https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/Japanese/NotoSansCJKjp-Regular.otf\n",
    "\n",
    "print(\"3. GitHubã‹ã‚‰é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ(stopwords.txt)ã‚’å–å¾—ã—ã¦ã„ã¾ã™ã€‚\")\n",
    "github_repo_url = \"https://github.com/amufaamo/blog-to-tagcloud\"\n",
    "stopwords_url = github_repo_url.replace('github.com', 'raw.githubusercontent.com') + '/main/stopwords.txt'\n",
    "!wget -q -O /content/stopwords.txt {stopwords_url}\n",
    "\n",
    "print(\"æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸï¼\\n\")\n",
    "\n",
    "# --- â–¼â–¼â–¼ ã“ã“ã‹ã‚‰ãŒæ–°ã—ã„åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ â–¼â–¼â–¼ ---\n",
    "PLATFORM_CONFIGS = {\n",
    "    'blogger': {\n",
    "        'post_container': 'article.post-outer-container',\n",
    "        'permalink': 'h3.post-title a',\n",
    "        'date': 'time.published',\n",
    "        'date_attribute': 'datetime',\n",
    "        'content_body': 'div.post-body',\n",
    "        'pagination': 'a.blog-pager-older-link',\n",
    "    },\n",
    "    'hatenablog': {\n",
    "        'post_container': 'article.entry',\n",
    "        'permalink': 'h1.entry-title a',\n",
    "        'date': 'time[datetime]',\n",
    "        'date_attribute': 'datetime',\n",
    "        'content_body': 'div.entry-content',\n",
    "        'pagination': 'a[rel=\"next\"]',\n",
    "    },\n",
    "    'ameblo': {\n",
    "        'post_container': 'li[data-unique-entry-id]',\n",
    "        'permalink': 'a[data-gtm-user-entry-title]',\n",
    "        'date': 'time',\n",
    "        'date_attribute': 'datetime',\n",
    "        'content_body': 'div[data-unique-entry-body]',\n",
    "        'pagination': 'a[data-gtm-button-name=\"è¨˜äº‹ä¸€è¦§_æ¬¡ã¸\"]',\n",
    "    },\n",
    "    'note': {\n",
    "        'post_container': 'div.o-cardNote',\n",
    "        'permalink': 'a.o-cardNote__link',\n",
    "        'date': 'time',\n",
    "        'date_attribute': 'datetime',\n",
    "        'content_body': 'div.note-common-styles__p',\n",
    "        'pagination': None, # ç„¡é™ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã®ãŸã‚\n",
    "    }\n",
    "}\n",
    "\n",
    "def detect_platform(url):\n",
    "    if 'hatenablog' in url: return 'hatenablog'\n",
    "    if 'ameblo.jp' in url: return 'ameblo'\n",
    "    if 'note.com' in url: return 'note'\n",
    "    if 'blogspot.com' in url: return 'blogger'\n",
    "    return None\n",
    "\n",
    "def load_stopwords(filepath='/content/stopwords.txt'):\n",
    "    if not os.path.exists(filepath): return set()\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        stopwords = {line.strip() for line in f if line.strip()}\n",
    "    print(f\"{len(stopwords)}å€‹ã®é™¤å¤–ãƒ¯ãƒ¼ãƒ‰ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚\")\n",
    "    return stopwords\n",
    "\n",
    "def analyze_blog(base_url, start_date, end_date, stopwords):\n",
    "    platform = detect_platform(base_url)\n",
    "    if not platform:\n",
    "        print(\"ã‚¨ãƒ©ãƒ¼: å¯¾å¿œã—ã¦ã„ãªã„ãƒ–ãƒ­ã‚°ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã™ã€‚ (Blogger, ã¯ã¦ãª, note, ã‚¢ãƒ¡ãƒ–ãƒ­ã«å¯¾å¿œ)\")\n",
    "        return None\n",
    "    print(f\"ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ : {platform.capitalize()} ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚\")\n",
    "    config = PLATFORM_CONFIGS[platform]\n",
    "    \n",
    "    all_text = \"\"\n",
    "    current_url = base_url\n",
    "    page_num = 1\n",
    "    \n",
    "    print(f\"ãƒ–ãƒ­ã‚°ã®åˆ†æã‚’é–‹å§‹ã—ã¾ã™ã€‚å¯¾è±¡æœŸé–“: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    while current_url:\n",
    "        print(f\"è¨˜äº‹ä¸€è¦§ãƒšãƒ¼ã‚¸ {page_num} ã‚’å–å¾—ä¸­: {current_url}\")\n",
    "        try:\n",
    "            response = requests.get(current_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            response.encoding = 'utf-8'\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            time.sleep(1) # ã‚µãƒ¼ãƒãƒ¼ã«å„ªã—ã\n",
    "        except:\n",
    "            print(\"ã‚¨ãƒ©ãƒ¼: ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚\")\n",
    "            break\n",
    "        \n",
    "        posts = soup.select(config['post_container'])\n",
    "        if not posts: \n",
    "            print(\"è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’çµ‚äº†ã—ã¾ã™ã€‚\")\n",
    "            break\n",
    "\n",
    "        stop_crawling = False\n",
    "        for post in posts:\n",
    "            date_tag = post.select_one(config['date'])\n",
    "            link_tag = post.select_one(config['permalink'])\n",
    "            \n",
    "            if not date_tag or not link_tag:\n",
    "                continue\n",
    "\n",
    "            post_date_str = date_tag.get(config['date_attribute'])\n",
    "            if not post_date_str: continue\n",
    "            \n",
    "            try:\n",
    "                post_date = datetime.fromisoformat(post_date_str.replace('Z', '+00:00'))\n",
    "            except ValueError:\n",
    "                continue # æ—¥ä»˜å½¢å¼ãŒä¸æ­£ãªå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—\n",
    "            \n",
    "            if start_date <= post_date <= end_date:\n",
    "                post_url = urljoin(base_url, link_tag['href'])\n",
    "                print(f\"  -> æœŸé–“å†…ã®è¨˜äº‹ã‚’ç™ºè¦‹({post_date.strftime('%Y-%m-%d')})ã€å†…å®¹ã‚’å–å¾—ä¸­: {post_url}\")\n",
    "                try:\n",
    "                    post_res = requests.get(post_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                    post_res.encoding = 'utf-8'\n",
    "                    post_soup = BeautifulSoup(post_res.text, 'html.parser')\n",
    "                    content_body = post_soup.select_one(config['content_body'])\n",
    "                    if content_body:\n",
    "                        all_text += content_body.get_text() + \"\\n\"\n",
    "                    time.sleep(1) # ã‚µãƒ¼ãƒãƒ¼ã«å„ªã—ã\n",
    "                except:\n",
    "                    print(f\"     -> è¨˜äº‹ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚\")\n",
    "\n",
    "            elif post_date < start_date:\n",
    "                stop_crawling = True\n",
    "                break\n",
    "        \n",
    "        if stop_crawling:\n",
    "            print(\"å¯¾è±¡æœŸé–“å¤–ã®è¨˜äº‹ã«åˆ°é”ã—ãŸãŸã‚ã€ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’çµ‚äº†ã—ã¾ã™ã€‚\")\n",
    "            break\n",
    "        \n",
    "        if config['pagination']:\n",
    "            next_page_tag = soup.select_one(config['pagination'])\n",
    "            if next_page_tag and next_page_tag.has_attr('href'):\n",
    "                current_url = urljoin(base_url, next_page_tag['href'])\n",
    "                page_num += 1\n",
    "            else:\n",
    "                current_url = None\n",
    "        else: # note.comãªã©ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ãŒãªã„å ´åˆ\n",
    "            print(f\"{platform.capitalize()}ã¯è¤‡æ•°ãƒšãƒ¼ã‚¸ã®ã‚¯ãƒ­ãƒ¼ãƒ«ã«éå¯¾å¿œã®ãŸã‚ã€æœ€åˆã®ãƒšãƒ¼ã‚¸ã®ã¿åˆ†æã—ã¾ã™ã€‚\")\n",
    "            current_url = None\n",
    "\n",
    "    if not all_text: return None\n",
    "    print(\"\\nãƒ†ã‚­ã‚¹ãƒˆã®è§£æä¸­...\")\n",
    "    t = Tokenizer()\n",
    "    words = [token.surface for token in t.tokenize(all_text) \n",
    "             if token.surface not in stopwords and \n",
    "             token.part_of_speech.startswith(('åè©', 'å‹•è©', 'å½¢å®¹è©')) and \n",
    "             len(token.surface) > 1 and not re.match(r'^[0-9a-zA-Z]+$', token.surface)]\n",
    "    return Counter(words)\n",
    "\n",
    "# ãƒ¡ã‚¤ãƒ³å‡¦ç†\n",
    "custom_stopwords = load_stopwords()\n",
    "word_counter = analyze_blog(blog_url, start_date_obj, end_date_obj, custom_stopwords)\n",
    "\n",
    "if word_counter:\n",
    "    print(\"\\n--- âœ…å˜èªã®é »åº¦ãƒ©ãƒ³ã‚­ãƒ³ã‚° TOP 50 ---\")\n",
    "    for word, count in word_counter.most_common(50):\n",
    "        print(f\"{word}: {count}å›\")\n",
    "    print(\"\\nãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ç”»åƒã‚’ç”Ÿæˆä¸­...\")\n",
    "    if os.path.exists(font_path) and os.path.getsize(font_path) > 1000000:\n",
    "      try:\n",
    "        wordcloud = WordCloud(width=1200, height=600, background_color='white', font_path=font_path, max_words=150, colormap='viridis').generate_from_frequencies(word_counter)\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "      except OSError as e:\n",
    "        print(f\"\\nã‚¨ãƒ©ãƒ¼: ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã®ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚\")\n",
    "    else:\n",
    "        print(f\"ã‚¨ãƒ©ãƒ¼ï¼šãƒ•ã‚©ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ããƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã€Œãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®å†èµ·å‹•ã€ã‚’è©¦ã—ã¦ãã ã•ã„ã€‚\")\n",
    "else:\n",
    "    print(\"ã‚¨ãƒ©ãƒ¼: åˆ†æå¯¾è±¡ã®è¨˜äº‹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚URLã‚„æœŸé–“ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
