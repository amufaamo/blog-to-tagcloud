{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blogger Word Cloud Generator 📝☁️\n",
    "\n",
    "分析したいブログのURLと期間（開始日・終了日）を入力して、下の▶ボタンを押してください。\n",
    "\n",
    "**注:** 除外したい単語は、GitHubリポジトリにある `stopwords.txt` ファイルを直接編集してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "forms": {
      " বিবিধ ": false,
      "সাইজ ": 1,
      "সময়ের বিন্যাস": false,
      "입력란": true
     }
    }
   },
   "outputs": [],
   "source": [
    "#@title ◆ 設定と実行 ◆\n",
    "#@markdown --- \n",
    "#@markdown ### 1. 分析したいブログの情報を入力\n",
    "#@markdown **分析したいブログのURLを入力してください**\n",
    "blog_url = \"\" #@param {type:\"string\"}\n",
    "#@markdown \n",
    "#@markdown --- \n",
    "#@markdown ### 2. 分析する期間を指定\n",
    "#@markdown **入力がない場合は、過去30日間の記事を分析します**\n",
    "start_date_str = \"\" #@param {type:\"date\"}\n",
    "end_date_str = \"\" #@param {type:\"date\"}\n",
    "\n",
    "#@markdown --- \n",
    "#@markdown **設定が終わったら、このセルを実行してください（左の▶ボタンをクリック）**\n",
    "import os\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "if not blog_url:\n",
    "    raise ValueError(\"ブログのURLが入力されていません。入力してから再度実行してください。\")\n",
    "\n",
    "# --- 日付のデフォルト値を設定 ---\n",
    "jst = timezone(timedelta(hours=9))\n",
    "today = datetime.now(jst)\n",
    "\n",
    "if end_date_str:\n",
    "    # 入力された日付の終わり（23:59:59）に設定\n",
    "    end_date_obj = datetime.strptime(end_date_str, '%Y-%m-%d').replace(hour=23, minute=59, second=59, tzinfo=jst)\n",
    "else:\n",
    "    # デフォルトは今日\n",
    "    end_date_obj = today\n",
    "\n",
    "if start_date_str:\n",
    "    # 入力された日付の始まり（00:00:00）に設定\n",
    "    start_date_obj = datetime.strptime(start_date_str, '%Y-%m-%d').replace(tzinfo=jst)\n",
    "else:\n",
    "    # デフォルトは終了日から30日前\n",
    "    start_date_obj = end_date_obj - timedelta(days=30)\n",
    "\n",
    "# --- 環境準備 ---\n",
    "github_repo_url = \"https://github.com/amufaamo/blog-to-tagcloud\"\n",
    "\n",
    "print(\"環境の準備中です...\")\n",
    "print(\"1. 必要なライブラリをインストールしています。\")\n",
    "!pip install requests beautifulsoup4 janome wordcloud matplotlib > /dev/null 2>&1\n",
    "\n",
    "print(\"2. 日本語フォントをダウンロードしています。\")\n",
    "font_path = '/content/NotoSansCJKjp-Regular.otf'\n",
    "if not os.path.exists(font_path) or os.path.getsize(font_path) < 1024*1024: \n",
    "    !wget -q -O {font_path} https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/Japanese/NotoSansCJKjp-Regular.otf\n",
    "\n",
    "print(\"3. GitHubから除外ワードリスト(stopwords.txt)を取得しています。\")\n",
    "stopwords_url = github_repo_url.replace('github.com', 'raw.githubusercontent.com') + '/main/stopwords.txt'\n",
    "!wget -q -O /content/stopwords.txt {stopwords_url}\n",
    "\n",
    "print(\"準備が完了しました！\\n\")\n",
    "\n",
    "# --- 以下、分析用のコード ---\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from janome.tokenizer import Tokenizer\n",
    "from collections import Counter\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_stopwords(filepath='/content/stopwords.txt'):\n",
    "    if not os.path.exists(filepath):\n",
    "        return set()\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        stopwords = {line.strip() for line in f if line.strip()}\n",
    "    print(f\"{len(stopwords)}個の除外ワードを読み込みました。\")\n",
    "    return stopwords\n",
    "\n",
    "def analyze_blog(base_url, start_date, end_date, stopwords):\n",
    "    all_text = \"\"\n",
    "    current_url = base_url\n",
    "    page_num = 1\n",
    "    print(f\"ブログの分析を開始します。\")\n",
    "    print(f\"対象期間: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    while current_url:\n",
    "        print(f\"ページ {page_num} を取得中...\")\n",
    "        try:\n",
    "            response = requests.get(current_url)\n",
    "            response.encoding = 'utf-8'\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        except:\n",
    "            print(\"エラー: ページにアクセスできませんでした。\")\n",
    "            break\n",
    "        posts = soup.find_all('article', class_='post-outer-container')\n",
    "        if not posts and page_num == 1: return None\n",
    "        \n",
    "        page_contains_valid_posts = False\n",
    "        stop_crawling = False\n",
    "        for post in posts:\n",
    "            time_tag = post.find('time', class_='published')\n",
    "            body_tag = post.find('div', class_='post-body')\n",
    "            if not time_tag or not body_tag: continue\n",
    "            \n",
    "            post_date = datetime.fromisoformat(time_tag['datetime'])\n",
    "            # 記事の日付が期間内かチェック\n",
    "            if start_date <= post_date <= end_date:\n",
    "                page_contains_valid_posts = True\n",
    "                all_text += body_tag.get_text() + \"\\n\"\n",
    "            # 記事が開始日より古ければ、これ以上さかのぼるのをやめる\n",
    "            elif post_date < start_date:\n",
    "                stop_crawling = True\n",
    "                break\n",
    "        \n",
    "        if stop_crawling:\n",
    "            print(\"対象期間外の記事に到達したため、クロールを終了します。\")\n",
    "            break\n",
    "\n",
    "        older_posts_link = soup.find('a', class_='blog-pager-older-link')\n",
    "        if older_posts_link and older_posts_link.has_attr('href'):\n",
    "            current_url = older_posts_link['href']\n",
    "            page_num += 1\n",
    "        else:\n",
    "            current_url = None\n",
    "            \n",
    "    if not all_text: return None\n",
    "    print(\"\\nテキストの解析中...\")\n",
    "    t = Tokenizer()\n",
    "    words = [token.surface for token in t.tokenize(all_text) \n",
    "             if token.surface not in stopwords and \n",
    "             token.part_of_speech.startswith(('名詞', '動詞', '形容詞')) and \n",
    "             len(token.surface) > 1 and not re.match(r'^[0-9a-zA-Z]+$', token.surface)]\n",
    "    return Counter(words)\n",
    "\n",
    "custom_stopwords = load_stopwords()\n",
    "word_counter = analyze_blog(blog_url, start_date_obj, end_date_obj, custom_stopwords)\n",
    "\n",
    "if word_counter:\n",
    "    print(\"\\n--- ✅単語の頻度ランキング TOP 50 ---\")\n",
    "    for word, count in word_counter.most_common(50):\n",
    "        print(f\"{word}: {count}回\")\n",
    "    \n",
    "    print(\"\\nワードクラウド画像を生成中...\")\n",
    "    if os.path.exists(font_path) and os.path.getsize(font_path) > 1000000:\n",
    "      try:\n",
    "        wordcloud = WordCloud(\n",
    "            width=1200, height=600, background_color='white',\n",
    "            font_path=font_path, max_words=150, colormap='viridis'\n",
    "        ).generate_from_frequencies(word_counter)\n",
    "        \n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "      except OSError as e:\n",
    "        print(f\"\\nエラー: ワードクラウドの生成中にエラーが発生しました。\")\n",
    "    else:\n",
    "        print(f\"エラー：フォントファイルが正しくダウンロードできませんでした。「ランタイムの再起動」を試してください。\")\n",
    "else:\n",
    "    print(\"エラー: 分析対象の記事が見つかりませんでした。URLや期間を確認してください。\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
