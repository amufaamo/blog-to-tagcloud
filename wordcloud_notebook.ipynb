{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ステップ1：環境準備\n",
    "\n",
    "まず、このセルを実行して、分析に必要なツールをすべてインストールします。\n",
    "\n",
    "**左の再生ボタン▶を押し、処理が終わるまで待ってください。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {}
    }
   },
   "outputs": [],
   "source": [
    "#@title 環境準備（ライブラリとフォントのインストール）\n",
    "import os\n",
    "\n",
    "print(\"--- 1. 必要なライブラリをインストールしています --- \")\n",
    "# ✅ メッセージをすべて表示するために、エラーを隠す設定を削除しました\n",
    "!pip install requests beautifulsoup4 janome wordcloud matplotlib\n",
    "\n",
    "print(\"\\n--- 2. 日本語フォントをダウンロードしています --- \")\n",
    "font_path = '/content/NotoSansCJKjp-Regular.otf'\n",
    "if not os.path.exists(font_path) or os.path.getsize(font_path) < 1024*1024: \n",
    "    !wget -O {font_path} https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/Japanese/NotoSansCJKjp-Regular.otf\n",
    "else:\n",
    "    print(\"フォントは既に存在します。\")\n",
    "\n",
    "print(\"\\n✅ 環境準備が完了しました。次のセルに進んでください。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ステップ2：設定と実行\n",
    "\n",
    "上のセルの実行が終わったら、ここにブログの情報を入力し、このセルを実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "forms": {
      " বিবিধ ": false,
      "সাইজ ": 1,
      "সময়ের বিন্যাস": false,
      "입력란": true
     }
    }
   },
   "outputs": [],
   "source": [
    "#@title ◆ 設定と実行 ◆\n",
    "#@markdown --- \n",
    "#@markdown ### 1. 分析したいブログの情報を入力\n",
    "blog_url = \"\" #@param {type:\"string\"}\n",
    "#@markdown \n",
    "#@markdown --- \n",
    "#@markdown ### 2. 分析する期間を指定\n",
    "base_date_str = \"\" #@param {type:\"date\"}\n",
    "days_to_go_back = 30 #@param {type:\"slider\", min:1, max:1000, step:1}\n",
    "\n",
    "#@markdown --- \n",
    "#@markdown **設定が終わったら、このセルを実行してください**\n",
    "import os\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from janome.tokenizer import Tokenizer\n",
    "from collections import Counter\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "if not blog_url:\n",
    "    raise ValueError(\"ブログのURLが入力されていません。\")\n",
    "\n",
    "jst = timezone(timedelta(hours=9))\n",
    "today = datetime.now(jst)\n",
    "if base_date_str:\n",
    "    end_date_obj = datetime.strptime(base_date_str, '%Y-%m-%d').replace(hour=23, minute=59, second=59, tzinfo=jst)\n",
    "else:\n",
    "    end_date_obj = today\n",
    "start_date_obj = end_date_obj - timedelta(days=days_to_go_back)\n",
    "font_path = '/content/NotoSansCJKjp-Regular.otf'\n",
    "github_repo_url = \"https://github.com/amufaamo/blog-to-tagcloud\"\n",
    "\n",
    "def load_stopwords(filepath='/content/stopwords.txt'):\n",
    "    if not os.path.exists(filepath):\n",
    "      print(\"stopwords.txtをダウンロードします。\")\n",
    "      stopwords_url = github_repo_url.replace('github.com', 'raw.githubusercontent.com') + '/main/stopwords.txt'\n",
    "      !wget -q -O {filepath} {stopwords_url}\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        stopwords = {line.strip() for line in f if line.strip()}\n",
    "    print(f\"{len(stopwords)}個の除外ワードを読み込みました。\")\n",
    "    return stopwords\n",
    "\n",
    "def analyze_blog(base_url, start_date, end_date, stopwords):\n",
    "    #... (この関数の内容は変更ありません) ...\n",
    "    platform = detect_platform(base_url)\n",
    "    if not platform: return None\n",
    "    config = PLATFORM_CONFIGS[platform]\n",
    "    all_text = \"\"\n",
    "    current_url = base_url\n",
    "    page_num = 1\n",
    "    print(f\"プラットフォーム: {platform.capitalize()} | 対象期間: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}\")\n",
    "    while current_url:\n",
    "        print(f\"記事一覧ページ {page_num} を取得中...\")\n",
    "        try:\n",
    "            response = requests.get(current_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            response.encoding = 'utf-8'\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            print(\"ページにアクセスできませんでした。\")\n",
    "            break\n",
    "        posts = soup.select(config['post_container'])\n",
    "        if not posts: break\n",
    "        stop_crawling = False\n",
    "        for post in posts:\n",
    "            date_tag = post.select_one(config['date'])\n",
    "            link_tag = post.select_one(config['permalink'])\n",
    "            if not date_tag or not link_tag: continue\n",
    "            post_date_str = date_tag.get(config['date_attribute'])\n",
    "            if not post_date_str: continue\n",
    "            try:\n",
    "                post_date = datetime.fromisoformat(post_date_str.replace('Z', '+00:00'))\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if start_date <= post_date <= end_date:\n",
    "                post_url = urljoin(base_url, link_tag['href'])\n",
    "                try:\n",
    "                    post_res = requests.get(post_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                    post_res.encoding = 'utf-8'\n",
    "                    post_soup = BeautifulSoup(post_res.text, 'html.parser')\n",
    "                    content_body = post_soup.select_one(config['content_body'])\n",
    "                    if content_body:\n",
    "                        all_text += content_body.get_text() + \"\\n\"\n",
    "                    time.sleep(1)\n",
    "                except:\n",
    "                    pass\n",
    "            elif post_date < start_date:\n",
    "                stop_crawling = True\n",
    "                break\n",
    "        if stop_crawling:\n",
    "            break\n",
    "        if config['pagination']:\n",
    "            next_page_tag = soup.select_one(config['pagination'])\n",
    "            if next_page_tag and next_page_tag.has_attr('href'):\n",
    "                current_url = urljoin(base_url, next_page_tag['href'])\n",
    "                page_num += 1\n",
    "            else:\n",
    "                current_url = None\n",
    "        else:\n",
    "            current_url = None\n",
    "    if not all_text: return None\n",
    "    print(\"\\nテキストの解析中...\")\n",
    "    t = Tokenizer()\n",
    "    words = [token.surface for token in t.tokenize(all_text) \n",
    "             if token.surface not in stopwords and \n",
    "             token.part_of_speech.startswith(('名詞', '動詞', '形容詞')) and \n",
    "             len(token.surface) > 1 and not re.match(r'^[0-9a-zA-Z]+$', token.surface)]\n",
    "    return Counter(words)\n",
    "\n",
    "PLATFORM_CONFIGS = {\n",
    "    'blogger': {'post_container': 'article.post-outer-container','permalink': 'h3.post-title a','date': 'time.published','date_attribute': 'datetime','content_body': 'div.post-body','pagination': 'a.blog-pager-older-link'},\n",
    "    'hatenablog': {'post_container': 'article.entry','permalink': 'h1.entry-title a','date': 'time[datetime]','date_attribute': 'datetime','content_body': 'div.entry-content','pagination': 'a[rel=\"next\"]'},\n",
    "    'ameblo': {'post_container': 'li[data-unique-entry-id]','permalink': 'a[data-gtm-user-entry-title]','date': 'time','date_attribute': 'datetime','content_body': 'div[data-unique-entry-body]','pagination': 'a[data-gtm-button-name=\"記事一覧_次へ\"]'},\n",
    "    'note': {'post_container': 'div.o-cardNote','permalink': 'a.o-cardNote__link','date': 'time','date_attribute': 'datetime','content_body': 'div.note-common-styles__p','pagination': None}\n",
    "}\n",
    "\n",
    "def detect_platform(url):\n",
    "    if 'hatenablog' in url: return 'hatenablog'\n",
    "    if 'ameblo.jp' in url: return 'ameblo'\n",
    "    if 'note.com' in url: return 'note'\n",
    "    if 'blogspot.com' in url: return 'blogger'\n",
    "    return None\n",
    "\n",
    "# メイン処理\n",
    "custom_stopwords = load_stopwords()\n",
    "word_counter = analyze_blog(blog_url, start_date_obj, end_date_obj, custom_stopwords)\n",
    "\n",
    "if word_counter:\n",
    "    print(\"\\n--- ✅単語の頻度ランキング TOP 50 ---\")\n",
    "    for word, count in word_counter.most_common(50):\n",
    "        print(f\"{word}: {count}回\")\n",
    "    print(\"\\nワードクラウド画像を生成中...\")\n",
    "    if os.path.exists(font_path):\n",
    "      try:\n",
    "        wordcloud = WordCloud(width=1200, height=600, background_color='white', font_path=font_path, max_words=150, colormap='viridis').generate_from_frequencies(word_counter)\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "      except Exception as e:\n",
    "        print(f\"\\nエラー: ワードクラウドの生成中にエラー: {e}\")\n",
    "    else:\n",
    "        print(f\"エラー：フォントファイルが見つかりません。ステップ1を再実行してください。\")\n",
    "else:\n",
    "    print(\"エラー: 分析対象の記事が見つかりませんでした。\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
