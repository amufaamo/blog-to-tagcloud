{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universal Blog Word Cloud Generator 📝☁️\n",
    "\n",
    "**使い方:**\n",
    "1. **ステップ1**でブログのURLと期間を設定します。\n",
    "2. **ステップ2**を実行して、ブログの全テキストを取得します（この処理は時間がかかります）。\n",
    "3. **ステップ3**を実行すると、最初の分析結果が表示されます。結果を見ながらテキストボックス内の除外ワードを編集し、何度でもステップ3を再実行して結果を調整できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ステップ1：分析対象の設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "forms": {
      " বিবিধ ": false,
      "সাইজ ": 1,
      "সময়ের বিন্যাস": false,
      "입력란": true
     }
    }
   },
   "outputs": [],
   "source": [
    "#@title ◆ ブログ情報と期間を入力 ◆\n",
    "#@markdown --- \n",
    "#@markdown **分析したいブログのURLを入力してください**\n",
    "blog_url = \"\" #@param {type:\"string\"}\n",
    "#@markdown \n",
    "#@markdown --- \n",
    "#@markdown **基準日（この日まで）を指定してください。空欄の場合は今日になります。**\n",
    "base_date_str = \"\" #@param {type:\"date\"}\n",
    "#@markdown **基準日から何日遡って分析するか指定してください。**\n",
    "days_to_go_back = 30 #@param {type:\"slider\", min:1, max:1000, step:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ステップ2：ブログ記事の取得（時間のかかる処理）\n",
    "\n",
    "設定が終わったら、下のセルを実行してブログの情報を取得します。この処理は一度だけ実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title ▼ 記事の取得を実行\n",
    "import os\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "\n",
    "print(\"環境の準備中です...\")\n",
    "!pip install requests beautifulsoup4 janome wordcloud matplotlib > /dev/null 2>&1\n",
    "font_path = '/content/NotoSansCJKjp-Regular.otf'\n",
    "if not os.path.exists(font_path) or os.path.getsize(font_path) < 1024*1024:\n",
    "    !wget -q -O {font_path} https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/Japanese/NotoSansCJKjp-Regular.otf\n",
    "github_repo_url = \"https://github.com/amufaamo/blog-to-tagcloud\"\n",
    "stopwords_path = '/content/stopwords.txt'\n",
    "stopwords_url = github_repo_url.replace('github.com', 'raw.githubusercontent.com') + '/main/stopwords.txt'\n",
    "!wget -q -O {stopwords_path} {stopwords_url}\n",
    "print(\"準備が完了しました！\\n\")\n",
    "\n",
    "PLATFORM_CONFIGS = {\n",
    "    'blogger': {'post_container': 'article.post-outer-container','permalink': 'h3.post-title a','date': 'time.published','date_attribute': 'datetime','content_body': 'div.post-body','pagination': 'a.blog-pager-older-link'},\n",
    "    'hatenablog': {'post_container': 'article.entry','permalink': 'h1.entry-title a, h2.entry-title a','date': 'time[datetime]','date_attribute': 'datetime','content_body': 'div.entry-content','pagination': 'a[rel=\"next\"]'},\n",
    "    'ameblo': {'post_container': 'li[data-unique-entry-id]','permalink': 'a[data-gtm-user-entry-title]','date': 'time','date_attribute': 'datetime','content_body': 'div[data-unique-entry-body]','pagination': 'a[data-gtm-button-name=\"記事一覧_次へ\"]'},\n",
    "    'note': {'post_container': 'div.o-cardNote','permalink': 'a.o-cardNote__link','date': 'time','date_attribute': 'datetime','content_body': 'div.note-common-styles__p','pagination': None}\n",
    "}\n",
    "\n",
    "def detect_platform(url, soup):\n",
    "    if 'hatenablog' in url or soup.select_one('link[href*=\"cdn.hatena.com\"]'): return 'hatenablog'\n",
    "    if 'ameblo.jp' in url or soup.select_one('meta[property=\"og:site_name\"][content=\"Ameba\"]'): return 'ameblo'\n",
    "    if 'note.com' in url: return 'note'\n",
    "    if 'blogspot.com' in url or soup.select_one('meta[content=\"blogger\"]'): return 'blogger'\n",
    "    return None\n",
    "\n",
    "def analyze_blog(base_url, start_date, end_date):\n",
    "    all_text = \"\"\n",
    "    current_url = base_url\n",
    "    page_num = 1\n",
    "    print(f\"ブログの分析を開始します。対象期間: {start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}\")\n",
    "    try:\n",
    "        response = requests.get(current_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        response.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    platform = detect_platform(base_url, soup)\n",
    "    if not platform: return None\n",
    "    print(f\"プラットフォーム: {platform.capitalize()} を検出しました。\")\n",
    "    config = PLATFORM_CONFIGS[platform]\n",
    "    while current_url:\n",
    "        if page_num > 1:\n",
    "            try:\n",
    "                response = requests.get(current_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                response.encoding = 'utf-8'\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            except:\n",
    "                break\n",
    "        time.sleep(1)\n",
    "        posts = soup.select(config['post_container'])\n",
    "        if not posts: break\n",
    "        stop_crawling = False\n",
    "        for post in posts:\n",
    "            date_tag = post.select_one(config['date'])\n",
    "            link_tag = post.select_one(config['permalink'])\n",
    "            if not date_tag or not link_tag: continue\n",
    "            post_date_str = date_tag.get(config['date_attribute'])\n",
    "            if not post_date_str: continue\n",
    "            try:\n",
    "                post_date = datetime.fromisoformat(post_date_str.replace('Z', '+00:00'))\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if start_date <= post_date <= end_date:\n",
    "                post_url = urljoin(base_url, link_tag['href'])\n",
    "                try:\n",
    "                    post_res = requests.get(post_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "                    post_res.encoding = 'utf-8'\n",
    "                    post_soup = BeautifulSoup(post_res.text, 'html.parser')\n",
    "                    content_body = post_soup.select_one(config['content_body'])\n",
    "                    if content_body:\n",
    "                        all_text += content_body.get_text() + \"\\n\"\n",
    "                    time.sleep(1)\n",
    "                except:\n",
    "                    pass\n",
    "            elif post_date < start_date:\n",
    "                stop_crawling = True\n",
    "                break\n",
    "        if stop_crawling:\n",
    "            break\n",
    "        if config['pagination']:\n",
    "            next_page_tag = soup.select_one(config['pagination'])\n",
    "            if next_page_tag and next_page_tag.has_attr('href'):\n",
    "                current_url = urljoin(base_url, next_page_tag['href'])\n",
    "                page_num += 1\n",
    "            else:\n",
    "                current_url = None\n",
    "        else:\n",
    "            current_url = None\n",
    "    return all_text\n",
    "\n",
    "# メイン処理\n",
    "if not blog_url: raise ValueError(\"ステップ1でブログのURLが入力されていません。\")\n",
    "jst = timezone(timedelta(hours=9))\n",
    "today = datetime.now(jst)\n",
    "if base_date_str: end_date_obj = datetime.strptime(base_date_str, '%Y-%m-%d').replace(hour=23, minute=59, second=59, tzinfo=jst)\n",
    "else: end_date_obj = today\n",
    "start_date_obj = end_date_obj - timedelta(days=days_to_go_back)\n",
    "\n",
    "scraped_text = analyze_blog(blog_url, start_date_obj, end_date_obj)\n",
    "if scraped_text:\n",
    "    print(\"\\n✅ ブログ記事の取得が完了しました。ステップ3に進んで分析を実行してください。\")\n",
    "else:\n",
    "    print(\"\\nエラー: 分析対象の記事が見つかりませんでした。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ステップ3：分析と対話的な再分析\n",
    "\n",
    "下のテキストボックスに、`stopwords.txt`から読み込んだ除外ワードが表示されます。リストを編集して**このセルを実行（▶）**すると、即座に結果が更新されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "forms": {
      " বিবিধ ": false,
      "সাইজ ": 1,
      "সময়ের বিন্যাস": false,
      "입력란": true
     }
    }
   },
   "outputs": [],
   "source": [
    "#@title ▼ 除外ワードを編集して分析・再分析\n",
    "from janome.tokenizer import Tokenizer\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "\n",
    "# ステップ2が実行済みかチェック\n",
    "if 'scraped_text' not in globals() or not scraped_text:\n",
    "    print(\"エラー：先にステップ2を実行して、ブログのテキストを取得してください。\")\n",
    "else:\n",
    "    # --- 除外ワードリストの準備 ---\n",
    "    # 初回実行時のみファイルから読み込み、テキストエリアのデフォルト値として利用します。\n",
    "    # 2回目以降の実行では、ユーザーが編集した内容がこのテキストエリアに維持されます。\n",
    "    try:\n",
    "        # このセルが初めて実行される時にだけ、ファイルから読み込む\n",
    "        if 'editable_stopwords' not in globals():\n",
    "            with open('/content/stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "                # editable_stopwords にファイル内容を保存\n",
    "                editable_stopwords = f.read()\n",
    "                print(\"stopwords.txtから除外ワードを読み込みました。\")\n",
    "    except FileNotFoundError:\n",
    "        # ファイルが見つからない場合（ステップ2が未実行など）\n",
    "        if 'editable_stopwords' not in globals():\n",
    "            editable_stopwords = \"\" # 空の状態で開始\n",
    "            print(\"警告: stopwords.txt が見つかりませんでした。\")\n",
    "\n",
    "    #@markdown ---\n",
    "    #@markdown **↓のリストを自由に編集し、このセル（▶）を再実行すると、結果が更新されます。**\n",
    "    #@markdown （1行に1単語の形式で追加・削除してください）\n",
    "    editable_stopwords = editable_stopwords #@param {type:\"string\"}\n",
    "\n",
    "    # --- テキストの分析 ---\n",
    "    def reanalyze_text(text, stopwords_str):\n",
    "        # テキストエリアの文字列を改行で分割し、除外ワードのセットを作成\n",
    "        stopwords = {line.strip() for line in stopwords_str.splitlines() if line.strip()}\n",
    "        print(f\"\\n{len(stopwords)}個の除外ワードでテキストを再分析中...\")\n",
    "        t = Tokenizer()\n",
    "        words = [token.surface for token in t.tokenize(text)\n",
    "                 if token.surface not in stopwords and\n",
    "                 token.part_of_speech.startswith(('名詞', '動詞', '形容詞')) and\n",
    "                 len(token.surface) > 1 and not re.match(r'^[0-9a-zA-Z]+$', token.surface)]\n",
    "        return Counter(words)\n",
    "\n",
    "    # --- 結果の表示 ---\n",
    "    def show_results(counter):\n",
    "        print(\"\\n--- ✅単語の頻度ランキング TOP 50 ---\")\n",
    "        for word, count in counter.most_common(50):\n",
    "            print(f\"{word}: {count}回\")\n",
    "\n",
    "        font_path = '/content/NotoSansCJKjp-Regular.otf'\n",
    "        # ステップ2でダウンロードしたフォントファイルの存在をチェック\n",
    "        if os.path.exists(font_path) and os.path.getsize(font_path) > 1000000:\n",
    "          print(\"\\nワードクラウド画像を生成中...\")\n",
    "          try:\n",
    "            wordcloud = WordCloud(width=1200, height=600, background_color='white', font_path=font_path, max_words=150, colormap='viridis').generate_from_frequencies(counter)\n",
    "            plt.figure(figsize=(15, 8))\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.axis(\"off\")\n",
    "            plt.show()\n",
    "          except Exception as e:\n",
    "            # ワードクラウド生成中にエラーが起きた場合\n",
    "            print(f\"\\nエラー: ワードクラウドの生成に失敗しました。詳細: {e}\")\n",
    "        else:\n",
    "            # フォントファイルが見つからない場合\n",
    "            print(\"\\nエラー：ワードクラウド生成用のフォントファイルが見つかりません。ステップ2を再実行してください。\")\n",
    "\n",
    "    # --- メイン処理 ---\n",
    "    # 編集された除外ワードを使って分析を実行\n",
    "    word_counter = reanalyze_text(scraped_text, editable_stopwords)\n",
    "    if word_counter:\n",
    "        show_results(word_counter)\n",
    "    else:\n",
    "        print(\"\\n分析の結果、有効な単語が見つかりませんでした。除外ワードリストを見直してみてください。\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
