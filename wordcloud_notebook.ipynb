{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blogger Word Cloud Generator 📝☁️\n",
    "\n",
    "ブログのURLと分析期間を入力して、下の▶ボタンを押してください。\n",
    "\n",
    "**注:** 除外したい単語は、GitHubリポジトリにある `stopwords.txt` ファイルを編集してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "forms": {
      " বিবিধ ": false,
      "সাইজ ": 1,
      "সময়ের বিন্যাস": false,
      "입력란": true
     }
    }
   },
   "outputs": [],
   "source": [
    "#@title ◆ 設定と実行 ◆\n",
    "#@markdown --- \n",
    "#@markdown ### 1. あなたのブログとリポジトリの情報を入力\n",
    "blog_url = \"https://amufaamo.blogspot.com/\" #@param {type:\"string\"}\n",
    "#@markdown GitHubリポジトリのURL (`https://github.com/ユーザー名/リポジトリ名`)\n",
    "github_repo_url = \"https://github.com/amufaamo/blog-to-tagcloud\" #@param {type:\"string\"}\n",
    "days_to_analyze = 30 #@param {type:\"slider\", min:7, max:1000, step:1}\n",
    "\n",
    "#@markdown --- \n",
    "#@markdown **設定が終わったら、このセルを実行してください（左の▶ボタンをクリック）**\n",
    "import os\n",
    "\n",
    "print(\"環境の準備中です...\")\n",
    "print(\"1. 必要なライブラリをインストールしています。\")\n",
    "!pip install requests beautifulsoup4 janome wordcloud matplotlib > /dev/null 2>&1\n",
    "\n",
    "print(\"2. 日本語フォントをダウンロードしています。\")\n",
    "font_path = '/content/NotoSansCJKjp-Regular.otf'\n",
    "if not os.path.exists(font_path):\n",
    "    !wget -q -O {font_path} https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/Japanese/NotoSansCJKjp-Regular.otf\n",
    "\n",
    "print(\"3. GitHubから除外ワードリスト(stopwords.txt)を取得しています。\")\n",
    "# GitHubのURLをRAWコンテンツのURLに変換\n",
    "stopwords_url = github_repo_url.replace('github.com', 'raw.githubusercontent.com') + '/main/stopwords.txt'\n",
    "!wget -q -O /content/stopwords.txt {stopwords_url}\n",
    "\n",
    "print(\"準備が完了しました！\\n\")\n",
    "\n",
    "# --- 以下、分析用のコード ---\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from janome.tokenizer import Tokenizer\n",
    "from collections import Counter\n",
    "import re\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_stopwords(filepath='/content/stopwords.txt'):\n",
    "    \"\"\"ファイルから除外ワードを読み込み、セットとして返す\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        return set()\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        stopwords = {line.strip() for line in f if line.strip()}\n",
    "    print(f\"{len(stopwords)}個の除外ワードを読み込みました。\")\n",
    "    return stopwords\n",
    "\n",
    "def analyze_blog(base_url, days_to_check, stopwords):\n",
    "    all_text = \"\"\n",
    "    jst = timezone(timedelta(hours=9))\n",
    "    start_date = datetime.now(jst) - timedelta(days=days_to_check)\n",
    "    current_url = base_url\n",
    "    page_num = 1\n",
    "    print(f\"ブログの分析を開始します。対象期間: {days_to_check}日間\")\n",
    "    while current_url:\n",
    "        print(f\"ページ {page_num} を取得中...\")\n",
    "        try:\n",
    "            response = requests.get(current_url)\n",
    "            response.encoding = 'utf-8'\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        except:\n",
    "            print(\"エラー: ページにアクセスできませんでした。\")\n",
    "            break\n",
    "        posts = soup.find_all('article', class_='post-outer-container')\n",
    "        if not posts and page_num == 1: return None\n",
    "        page_contains_valid_posts = False\n",
    "        for post in posts:\n",
    "            time_tag = post.find('time', class_='published')\n",
    "            body_tag = post.find('div', class_='post-body')\n",
    "            if not time_tag or not body_tag: continue\n",
    "            post_date = datetime.fromisoformat(time_tag['datetime'])\n",
    "            if post_date >= start_date:\n",
    "                page_contains_valid_posts = True\n",
    "                all_text += body_tag.get_text() + \"\\n\"\n",
    "        if not page_contains_valid_posts and page_num > 1: break\n",
    "        older_posts_link = soup.find('a', class_='blog-pager-older-link')\n",
    "        if older_posts_link and older_posts_link.has_attr('href'):\n",
    "            current_url = older_posts_link['href']\n",
    "            page_num += 1\n",
    "        else:\n",
    "            current_url = None\n",
    "    if not all_text: return None\n",
    "    print(\"\\nテキストの解析中...\")\n",
    "    t = Tokenizer()\n",
    "    words = [token.surface for token in t.tokenize(all_text) \n",
    "             if token.surface not in stopwords and \n",
    "             token.part_of_speech.startswith(('名詞', '動詞', '形容詞')) and \n",
    "             len(token.surface) > 1 and not re.match(r'^[0-9a-zA-Z]+$', token.surface)]\n",
    "    return Counter(words)\n",
    "\n",
    "# メイン処理\n",
    "custom_stopwords = load_stopwords()\n",
    "word_counter = analyze_blog(blog_url, days_to_analyze, custom_stopwords)\n",
    "\n",
    "if word_counter:\n",
    "    print(\"\\n--- ✅単語の頻度ランキング TOP 50 ---\")\n",
    "    for word, count in word_counter.most_common(50):\n",
    "        print(f\"{word}: {count}回\")\n",
    "    \n",
    "    print(\"\\nワードクラウド画像を生成中...\")\n",
    "    if os.path.exists(font_path) and os.path.getsize(font_path) > 1000000:\n",
    "      try:\n",
    "        wordcloud = WordCloud(\n",
    "            width=1200, height=600, background_color='white',\n",
    "            font_path=font_path, max_words=150, colormap='viridis'\n",
    "        ).generate_from_frequencies(word_counter)\n",
    "        \n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "      except OSError as e:\n",
    "        print(f\"\\nエラー: ワードクラウドの生成中にエラーが発生しました。フォントファイルが破損している可能性があります。\")\n",
    "        print(f\"エラー詳細: {e}\")\n",
    "    else:\n",
    "        print(f\"エラー：フォントファイルが正しくダウンロードできませんでした。もう一度セルを実行するか、「ランタイムの再起動」を試してください。\")\n",
    "else:\n",
    "    print(\"エラー: 分析対象の記事が見つかりませんでした。URLや期間を確認してください。\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
